{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielbehargithub/LinkedIn_Salary/blob/main/matching/Matching_Algorithm_updated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9svrQOtPTyY"
      },
      "source": [
        "## CV Job Matching using Doc2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EF1ia-RPTyb"
      },
      "source": [
        "### Coding\n",
        "#### 1. Set up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDFUjQi3S171",
        "outputId": "80fa5d67-07cb-4db7-baab-724af8b8a270"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2024.12.14)\n",
            "Collecting playwright\n",
            "  Downloading playwright-1.49.1-py3-none-manylinux1_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: greenlet==3.1.1 in /usr/local/lib/python3.11/dist-packages (from playwright) (3.1.1)\n",
            "Collecting pyee==12.0.0 (from playwright)\n",
            "  Downloading pyee-12.0.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from pyee==12.0.0->playwright) (4.12.2)\n",
            "Downloading playwright-1.49.1-py3-none-manylinux1_x86_64.whl (44.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyee-12.0.0-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pyee, playwright\n",
            "Successfully installed playwright-1.49.1 pyee-12.0.0\n",
            "Downloading Chromium 131.0.6778.33 (playwright build v1148)\u001b[2m from https://playwright.azureedge.net/builds/chromium/1148/chromium-linux.zip\u001b[22m\n",
            "\u001b[1G161.3 MiB [] 0% 0.0s\u001b[0K\u001b[1G161.3 MiB [] 0% 35.3s\u001b[0K\u001b[1G161.3 MiB [] 0% 28.4s\u001b[0K\u001b[1G161.3 MiB [] 0% 16.8s\u001b[0K\u001b[1G161.3 MiB [] 0% 10.4s\u001b[0K\u001b[1G161.3 MiB [] 1% 7.1s\u001b[0K\u001b[1G161.3 MiB [] 1% 8.0s\u001b[0K\u001b[1G161.3 MiB [] 1% 7.6s\u001b[0K\u001b[1G161.3 MiB [] 1% 7.0s\u001b[0K\u001b[1G161.3 MiB [] 2% 7.0s\u001b[0K\u001b[1G161.3 MiB [] 2% 6.8s\u001b[0K\u001b[1G161.3 MiB [] 2% 6.9s\u001b[0K\u001b[1G161.3 MiB [] 2% 7.0s\u001b[0K\u001b[1G161.3 MiB [] 3% 7.1s\u001b[0K\u001b[1G161.3 MiB [] 3% 7.4s\u001b[0K\u001b[1G161.3 MiB [] 3% 7.1s\u001b[0K\u001b[1G161.3 MiB [] 3% 6.6s\u001b[0K\u001b[1G161.3 MiB [] 4% 6.4s\u001b[0K\u001b[1G161.3 MiB [] 4% 6.6s\u001b[0K\u001b[1G161.3 MiB [] 4% 6.3s\u001b[0K\u001b[1G161.3 MiB [] 5% 6.0s\u001b[0K\u001b[1G161.3 MiB [] 5% 5.8s\u001b[0K\u001b[1G161.3 MiB [] 6% 5.7s\u001b[0K\u001b[1G161.3 MiB [] 6% 5.8s\u001b[0K\u001b[1G161.3 MiB [] 6% 6.2s\u001b[0K\u001b[1G161.3 MiB [] 7% 6.0s\u001b[0K\u001b[1G161.3 MiB [] 8% 5.8s\u001b[0K\u001b[1G161.3 MiB [] 8% 5.6s\u001b[0K\u001b[1G161.3 MiB [] 9% 5.4s\u001b[0K\u001b[1G161.3 MiB [] 9% 5.5s\u001b[0K\u001b[1G161.3 MiB [] 10% 5.5s\u001b[0K\u001b[1G161.3 MiB [] 10% 5.4s\u001b[0K\u001b[1G161.3 MiB [] 11% 5.5s\u001b[0K\u001b[1G161.3 MiB [] 11% 5.6s\u001b[0K\u001b[1G161.3 MiB [] 12% 5.6s\u001b[0K\u001b[1G161.3 MiB [] 12% 5.5s\u001b[0K\u001b[1G161.3 MiB [] 13% 5.3s\u001b[0K\u001b[1G161.3 MiB [] 14% 5.2s\u001b[0K\u001b[1G161.3 MiB [] 14% 5.3s\u001b[0K\u001b[1G161.3 MiB [] 15% 5.2s\u001b[0K\u001b[1G161.3 MiB [] 15% 5.1s\u001b[0K\u001b[1G161.3 MiB [] 16% 5.0s\u001b[0K\u001b[1G161.3 MiB [] 17% 4.9s\u001b[0K\u001b[1G161.3 MiB [] 18% 4.9s\u001b[0K\u001b[1G161.3 MiB [] 18% 4.8s\u001b[0K\u001b[1G161.3 MiB [] 19% 4.7s\u001b[0K\u001b[1G161.3 MiB [] 19% 4.6s\u001b[0K\u001b[1G161.3 MiB [] 20% 4.6s\u001b[0K\u001b[1G161.3 MiB [] 21% 4.5s\u001b[0K\u001b[1G161.3 MiB [] 22% 4.4s\u001b[0K\u001b[1G161.3 MiB [] 22% 4.3s\u001b[0K\u001b[1G161.3 MiB [] 23% 4.3s\u001b[0K\u001b[1G161.3 MiB [] 24% 4.3s\u001b[0K\u001b[1G161.3 MiB [] 24% 4.2s\u001b[0K\u001b[1G161.3 MiB [] 25% 4.1s\u001b[0K\u001b[1G161.3 MiB [] 26% 4.1s\u001b[0K\u001b[1G161.3 MiB [] 26% 4.0s\u001b[0K\u001b[1G161.3 MiB [] 27% 4.0s\u001b[0K\u001b[1G161.3 MiB [] 27% 3.9s\u001b[0K\u001b[1G161.3 MiB [] 28% 3.8s\u001b[0K\u001b[1G161.3 MiB [] 29% 3.8s\u001b[0K\u001b[1G161.3 MiB [] 29% 3.9s\u001b[0K\u001b[1G161.3 MiB [] 30% 3.8s\u001b[0K\u001b[1G161.3 MiB [] 31% 3.7s\u001b[0K\u001b[1G161.3 MiB [] 32% 3.6s\u001b[0K\u001b[1G161.3 MiB [] 33% 3.5s\u001b[0K\u001b[1G161.3 MiB [] 33% 3.4s\u001b[0K\u001b[1G161.3 MiB [] 34% 3.4s\u001b[0K\u001b[1G161.3 MiB [] 34% 3.3s\u001b[0K\u001b[1G161.3 MiB [] 35% 3.3s\u001b[0K\u001b[1G161.3 MiB [] 36% 3.2s\u001b[0K\u001b[1G161.3 MiB [] 37% 3.1s\u001b[0K\u001b[1G161.3 MiB [] 38% 3.0s\u001b[0K\u001b[1G161.3 MiB [] 39% 3.0s\u001b[0K\u001b[1G161.3 MiB [] 39% 2.9s\u001b[0K\u001b[1G161.3 MiB [] 40% 2.9s\u001b[0K\u001b[1G161.3 MiB [] 41% 2.8s\u001b[0K\u001b[1G161.3 MiB [] 42% 2.7s\u001b[0K\u001b[1G161.3 MiB [] 43% 2.7s\u001b[0K\u001b[1G161.3 MiB [] 43% 2.6s\u001b[0K\u001b[1G161.3 MiB [] 44% 2.6s\u001b[0K\u001b[1G161.3 MiB [] 44% 2.5s\u001b[0K\u001b[1G161.3 MiB [] 45% 2.5s\u001b[0K\u001b[1G161.3 MiB [] 46% 2.4s\u001b[0K\u001b[1G161.3 MiB [] 47% 2.4s\u001b[0K\u001b[1G161.3 MiB [] 47% 2.3s\u001b[0K\u001b[1G161.3 MiB [] 48% 2.3s\u001b[0K\u001b[1G161.3 MiB [] 49% 2.3s\u001b[0K\u001b[1G161.3 MiB [] 50% 2.2s\u001b[0K\u001b[1G161.3 MiB [] 51% 2.2s\u001b[0K\u001b[1G161.3 MiB [] 52% 2.1s\u001b[0K\u001b[1G161.3 MiB [] 53% 2.1s\u001b[0K\u001b[1G161.3 MiB [] 53% 2.0s\u001b[0K\u001b[1G161.3 MiB [] 54% 2.0s\u001b[0K\u001b[1G161.3 MiB [] 55% 2.0s\u001b[0K\u001b[1G161.3 MiB [] 55% 1.9s\u001b[0K\u001b[1G161.3 MiB [] 56% 1.9s\u001b[0K\u001b[1G161.3 MiB [] 57% 1.9s\u001b[0K\u001b[1G161.3 MiB [] 57% 1.8s\u001b[0K\u001b[1G161.3 MiB [] 58% 1.8s\u001b[0K\u001b[1G161.3 MiB [] 59% 1.8s\u001b[0K\u001b[1G161.3 MiB [] 60% 1.7s\u001b[0K\u001b[1G161.3 MiB [] 61% 1.7s\u001b[0K\u001b[1G161.3 MiB [] 62% 1.6s\u001b[0K\u001b[1G161.3 MiB [] 63% 1.6s\u001b[0K\u001b[1G161.3 MiB [] 64% 1.6s\u001b[0K\u001b[1G161.3 MiB [] 65% 1.6s\u001b[0K\u001b[1G161.3 MiB [] 65% 1.5s\u001b[0K\u001b[1G161.3 MiB [] 66% 1.5s\u001b[0K\u001b[1G161.3 MiB [] 67% 1.4s\u001b[0K\u001b[1G161.3 MiB [] 68% 1.4s\u001b[0K\u001b[1G161.3 MiB [] 69% 1.4s\u001b[0K\u001b[1G161.3 MiB [] 69% 1.3s\u001b[0K\u001b[1G161.3 MiB [] 70% 1.3s\u001b[0K\u001b[1G161.3 MiB [] 71% 1.2s\u001b[0K\u001b[1G161.3 MiB [] 72% 1.2s\u001b[0K\u001b[1G161.3 MiB [] 73% 1.1s\u001b[0K\u001b[1G161.3 MiB [] 74% 1.1s\u001b[0K\u001b[1G161.3 MiB [] 75% 1.0s\u001b[0K\u001b[1G161.3 MiB [] 76% 1.0s\u001b[0K\u001b[1G161.3 MiB [] 77% 1.0s\u001b[0K\u001b[1G161.3 MiB [] 77% 0.9s\u001b[0K\u001b[1G161.3 MiB [] 78% 0.9s\u001b[0K\u001b[1G161.3 MiB [] 79% 0.9s\u001b[0K\u001b[1G161.3 MiB [] 80% 0.8s\u001b[0K\u001b[1G161.3 MiB [] 81% 0.8s\u001b[0K\u001b[1G161.3 MiB [] 82% 0.7s\u001b[0K\u001b[1G161.3 MiB [] 83% 0.7s\u001b[0K\u001b[1G161.3 MiB [] 84% 0.6s\u001b[0K\u001b[1G161.3 MiB [] 85% 0.6s\u001b[0K\u001b[1G161.3 MiB [] 86% 0.6s\u001b[0K\u001b[1G161.3 MiB [] 86% 0.5s\u001b[0K\u001b[1G161.3 MiB [] 87% 0.5s\u001b[0K\u001b[1G161.3 MiB [] 88% 0.5s\u001b[0K\u001b[1G161.3 MiB [] 89% 0.4s\u001b[0K\u001b[1G161.3 MiB [] 90% 0.4s\u001b[0K\u001b[1G161.3 MiB [] 91% 0.4s\u001b[0K\u001b[1G161.3 MiB [] 91% 0.3s\u001b[0K\u001b[1G161.3 MiB [] 92% 0.3s\u001b[0K\u001b[1G161.3 MiB [] 93% 0.3s\u001b[0K\u001b[1G161.3 MiB [] 93% 0.2s\u001b[0K\u001b[1G161.3 MiB [] 94% 0.2s\u001b[0K\u001b[1G161.3 MiB [] 95% 0.2s\u001b[0K\u001b[1G161.3 MiB [] 96% 0.1s\u001b[0K\u001b[1G161.3 MiB [] 97% 0.1s\u001b[0K\u001b[1G161.3 MiB [] 98% 0.1s\u001b[0K\u001b[1G161.3 MiB [] 99% 0.0s\u001b[0K\u001b[1G161.3 MiB [] 100% 0.0s\u001b[0K\n",
            "Chromium 131.0.6778.33 (playwright build v1148) downloaded to /root/.cache/ms-playwright/chromium-1148\n",
            "Downloading Chromium Headless Shell 131.0.6778.33 (playwright build v1148)\u001b[2m from https://playwright.azureedge.net/builds/chromium/1148/chromium-headless-shell-linux.zip\u001b[22m\n",
            "\u001b[1G100.9 MiB [] 0% 0.0s\u001b[0K\u001b[1G100.9 MiB [] 0% 20.5s\u001b[0K\u001b[1G100.9 MiB [] 0% 15.1s\u001b[0K\u001b[1G100.9 MiB [] 0% 9.9s\u001b[0K\u001b[1G100.9 MiB [] 1% 6.3s\u001b[0K\u001b[1G100.9 MiB [] 2% 3.6s\u001b[0K\u001b[1G100.9 MiB [] 3% 2.8s\u001b[0K\u001b[1G100.9 MiB [] 4% 2.5s\u001b[0K\u001b[1G100.9 MiB [] 5% 2.5s\u001b[0K\u001b[1G100.9 MiB [] 6% 2.4s\u001b[0K\u001b[1G100.9 MiB [] 7% 2.3s\u001b[0K\u001b[1G100.9 MiB [] 7% 2.4s\u001b[0K\u001b[1G100.9 MiB [] 8% 2.4s\u001b[0K\u001b[1G100.9 MiB [] 9% 2.4s\u001b[0K\u001b[1G100.9 MiB [] 9% 2.3s\u001b[0K\u001b[1G100.9 MiB [] 10% 2.3s\u001b[0K\u001b[1G100.9 MiB [] 10% 2.5s\u001b[0K\u001b[1G100.9 MiB [] 11% 2.5s\u001b[0K\u001b[1G100.9 MiB [] 11% 2.7s\u001b[0K\u001b[1G100.9 MiB [] 12% 2.6s\u001b[0K\u001b[1G100.9 MiB [] 13% 2.6s\u001b[0K\u001b[1G100.9 MiB [] 14% 2.5s\u001b[0K\u001b[1G100.9 MiB [] 15% 2.5s\u001b[0K\u001b[1G100.9 MiB [] 15% 2.4s\u001b[0K\u001b[1G100.9 MiB [] 15% 2.5s\u001b[0K\u001b[1G100.9 MiB [] 16% 2.4s\u001b[0K\u001b[1G100.9 MiB [] 17% 2.4s\u001b[0K\u001b[1G100.9 MiB [] 17% 2.5s\u001b[0K\u001b[1G100.9 MiB [] 18% 2.4s\u001b[0K\u001b[1G100.9 MiB [] 19% 2.4s\u001b[0K\u001b[1G100.9 MiB [] 20% 2.3s\u001b[0K\u001b[1G100.9 MiB [] 21% 2.2s\u001b[0K\u001b[1G100.9 MiB [] 22% 2.1s\u001b[0K\u001b[1G100.9 MiB [] 23% 2.1s\u001b[0K\u001b[1G100.9 MiB [] 23% 2.2s\u001b[0K\u001b[1G100.9 MiB [] 24% 2.2s\u001b[0K\u001b[1G100.9 MiB [] 25% 2.1s\u001b[0K\u001b[1G100.9 MiB [] 26% 2.1s\u001b[0K\u001b[1G100.9 MiB [] 27% 2.0s\u001b[0K\u001b[1G100.9 MiB [] 28% 2.0s\u001b[0K\u001b[1G100.9 MiB [] 29% 1.9s\u001b[0K\u001b[1G100.9 MiB [] 30% 1.9s\u001b[0K\u001b[1G100.9 MiB [] 31% 1.8s\u001b[0K\u001b[1G100.9 MiB [] 32% 1.8s\u001b[0K\u001b[1G100.9 MiB [] 34% 1.7s\u001b[0K\u001b[1G100.9 MiB [] 35% 1.6s\u001b[0K\u001b[1G100.9 MiB [] 36% 1.6s\u001b[0K\u001b[1G100.9 MiB [] 37% 1.5s\u001b[0K\u001b[1G100.9 MiB [] 38% 1.5s\u001b[0K\u001b[1G100.9 MiB [] 39% 1.5s\u001b[0K\u001b[1G100.9 MiB [] 40% 1.5s\u001b[0K\u001b[1G100.9 MiB [] 41% 1.4s\u001b[0K\u001b[1G100.9 MiB [] 42% 1.4s\u001b[0K\u001b[1G100.9 MiB [] 43% 1.4s\u001b[0K\u001b[1G100.9 MiB [] 44% 1.3s\u001b[0K\u001b[1G100.9 MiB [] 45% 1.3s\u001b[0K\u001b[1G100.9 MiB [] 46% 1.3s\u001b[0K\u001b[1G100.9 MiB [] 47% 1.3s\u001b[0K\u001b[1G100.9 MiB [] 48% 1.2s\u001b[0K\u001b[1G100.9 MiB [] 49% 1.2s\u001b[0K\u001b[1G100.9 MiB [] 50% 1.2s\u001b[0K\u001b[1G100.9 MiB [] 51% 1.1s\u001b[0K\u001b[1G100.9 MiB [] 53% 1.1s\u001b[0K\u001b[1G100.9 MiB [] 54% 1.0s\u001b[0K\u001b[1G100.9 MiB [] 55% 1.0s\u001b[0K\u001b[1G100.9 MiB [] 57% 1.0s\u001b[0K\u001b[1G100.9 MiB [] 58% 0.9s\u001b[0K\u001b[1G100.9 MiB [] 59% 0.9s\u001b[0K\u001b[1G100.9 MiB [] 60% 0.9s\u001b[0K\u001b[1G100.9 MiB [] 61% 0.8s\u001b[0K\u001b[1G100.9 MiB [] 62% 0.8s\u001b[0K\u001b[1G100.9 MiB [] 64% 0.8s\u001b[0K\u001b[1G100.9 MiB [] 65% 0.7s\u001b[0K\u001b[1G100.9 MiB [] 66% 0.7s\u001b[0K\u001b[1G100.9 MiB [] 68% 0.7s\u001b[0K\u001b[1G100.9 MiB [] 69% 0.6s\u001b[0K\u001b[1G100.9 MiB [] 70% 0.6s\u001b[0K\u001b[1G100.9 MiB [] 71% 0.6s\u001b[0K\u001b[1G100.9 MiB [] 72% 0.6s\u001b[0K\u001b[1G100.9 MiB [] 73% 0.5s\u001b[0K\u001b[1G100.9 MiB [] 75% 0.5s\u001b[0K\u001b[1G100.9 MiB [] 76% 0.5s\u001b[0K\u001b[1G100.9 MiB [] 78% 0.4s\u001b[0K\u001b[1G100.9 MiB [] 79% 0.4s\u001b[0K\u001b[1G100.9 MiB [] 80% 0.4s\u001b[0K\u001b[1G100.9 MiB [] 81% 0.4s\u001b[0K\u001b[1G100.9 MiB [] 82% 0.3s\u001b[0K\u001b[1G100.9 MiB [] 84% 0.3s\u001b[0K\u001b[1G100.9 MiB [] 85% 0.3s\u001b[0K\u001b[1G100.9 MiB [] 87% 0.2s\u001b[0K\u001b[1G100.9 MiB [] 88% 0.2s\u001b[0K\u001b[1G100.9 MiB [] 89% 0.2s\u001b[0K\u001b[1G100.9 MiB [] 91% 0.2s\u001b[0K\u001b[1G100.9 MiB [] 92% 0.1s\u001b[0K\u001b[1G100.9 MiB [] 94% 0.1s\u001b[0K\u001b[1G100.9 MiB [] 95% 0.1s\u001b[0K\u001b[1G100.9 MiB [] 97% 0.1s\u001b[0K\u001b[1G100.9 MiB [] 99% 0.0s\u001b[0K\u001b[1G100.9 MiB [] 100% 0.0s\u001b[0K\n",
            "Chromium Headless Shell 131.0.6778.33 (playwright build v1148) downloaded to /root/.cache/ms-playwright/chromium_headless_shell-1148\n",
            "Downloading Firefox 132.0 (playwright build v1466)\u001b[2m from https://playwright.azureedge.net/builds/firefox/1466/firefox-ubuntu-22.04.zip\u001b[22m\n",
            "\u001b[1G87.6 MiB [] 0% 0.0s\u001b[0K\u001b[1G87.6 MiB [] 0% 22.6s\u001b[0K\u001b[1G87.6 MiB [] 0% 14.2s\u001b[0K\u001b[1G87.6 MiB [] 0% 10.2s\u001b[0K\u001b[1G87.6 MiB [] 1% 5.3s\u001b[0K\u001b[1G87.6 MiB [] 2% 3.1s\u001b[0K\u001b[1G87.6 MiB [] 4% 2.4s\u001b[0K\u001b[1G87.6 MiB [] 5% 2.0s\u001b[0K\u001b[1G87.6 MiB [] 6% 1.8s\u001b[0K\u001b[1G87.6 MiB [] 8% 1.5s\u001b[0K\u001b[1G87.6 MiB [] 10% 1.4s\u001b[0K\u001b[1G87.6 MiB [] 11% 1.3s\u001b[0K\u001b[1G87.6 MiB [] 12% 1.4s\u001b[0K\u001b[1G87.6 MiB [] 13% 1.4s\u001b[0K\u001b[1G87.6 MiB [] 14% 1.3s\u001b[0K\u001b[1G87.6 MiB [] 16% 1.2s\u001b[0K\u001b[1G87.6 MiB [] 18% 1.2s\u001b[0K\u001b[1G87.6 MiB [] 19% 1.2s\u001b[0K\u001b[1G87.6 MiB [] 20% 1.2s\u001b[0K\u001b[1G87.6 MiB [] 22% 1.1s\u001b[0K\u001b[1G87.6 MiB [] 23% 1.1s\u001b[0K\u001b[1G87.6 MiB [] 25% 1.0s\u001b[0K\u001b[1G87.6 MiB [] 26% 1.0s\u001b[0K\u001b[1G87.6 MiB [] 28% 1.0s\u001b[0K\u001b[1G87.6 MiB [] 30% 0.9s\u001b[0K\u001b[1G87.6 MiB [] 32% 0.9s\u001b[0K\u001b[1G87.6 MiB [] 34% 0.8s\u001b[0K\u001b[1G87.6 MiB [] 35% 0.8s\u001b[0K\u001b[1G87.6 MiB [] 36% 0.8s\u001b[0K\u001b[1G87.6 MiB [] 38% 0.8s\u001b[0K\u001b[1G87.6 MiB [] 39% 0.8s\u001b[0K\u001b[1G87.6 MiB [] 40% 0.8s\u001b[0K\u001b[1G87.6 MiB [] 41% 0.7s\u001b[0K\u001b[1G87.6 MiB [] 42% 0.7s\u001b[0K\u001b[1G87.6 MiB [] 43% 0.7s\u001b[0K\u001b[1G87.6 MiB [] 44% 0.7s\u001b[0K\u001b[1G87.6 MiB [] 45% 0.7s\u001b[0K\u001b[1G87.6 MiB [] 47% 0.7s\u001b[0K\u001b[1G87.6 MiB [] 48% 0.7s\u001b[0K\u001b[1G87.6 MiB [] 49% 0.7s\u001b[0K\u001b[1G87.6 MiB [] 50% 0.6s\u001b[0K\u001b[1G87.6 MiB [] 52% 0.6s\u001b[0K\u001b[1G87.6 MiB [] 53% 0.6s\u001b[0K\u001b[1G87.6 MiB [] 54% 0.7s\u001b[0K\u001b[1G87.6 MiB [] 56% 0.6s\u001b[0K\u001b[1G87.6 MiB [] 57% 0.6s\u001b[0K\u001b[1G87.6 MiB [] 59% 0.6s\u001b[0K\u001b[1G87.6 MiB [] 61% 0.5s\u001b[0K\u001b[1G87.6 MiB [] 63% 0.5s\u001b[0K\u001b[1G87.6 MiB [] 64% 0.5s\u001b[0K\u001b[1G87.6 MiB [] 66% 0.5s\u001b[0K\u001b[1G87.6 MiB [] 67% 0.4s\u001b[0K\u001b[1G87.6 MiB [] 69% 0.4s\u001b[0K\u001b[1G87.6 MiB [] 71% 0.4s\u001b[0K\u001b[1G87.6 MiB [] 72% 0.4s\u001b[0K\u001b[1G87.6 MiB [] 74% 0.3s\u001b[0K\u001b[1G87.6 MiB [] 76% 0.3s\u001b[0K\u001b[1G87.6 MiB [] 77% 0.3s\u001b[0K\u001b[1G87.6 MiB [] 80% 0.2s\u001b[0K\u001b[1G87.6 MiB [] 82% 0.2s\u001b[0K\u001b[1G87.6 MiB [] 84% 0.2s\u001b[0K\u001b[1G87.6 MiB [] 85% 0.2s\u001b[0K\u001b[1G87.6 MiB [] 87% 0.2s\u001b[0K\u001b[1G87.6 MiB [] 89% 0.1s\u001b[0K\u001b[1G87.6 MiB [] 91% 0.1s\u001b[0K\u001b[1G87.6 MiB [] 93% 0.1s\u001b[0K\u001b[1G87.6 MiB [] 94% 0.1s\u001b[0K\u001b[1G87.6 MiB [] 96% 0.0s\u001b[0K\u001b[1G87.6 MiB [] 97% 0.0s\u001b[0K\u001b[1G87.6 MiB [] 99% 0.0s\u001b[0K\u001b[1G87.6 MiB [] 100% 0.0s\u001b[0K\n",
            "Firefox 132.0 (playwright build v1466) downloaded to /root/.cache/ms-playwright/firefox-1466\n",
            "Downloading Webkit 18.2 (playwright build v2104)\u001b[2m from https://playwright.azureedge.net/builds/webkit/2104/webkit-ubuntu-22.04.zip\u001b[22m\n",
            "\u001b[1G95.5 MiB [] 0% 0.0s\u001b[0K\u001b[1G95.5 MiB [] 0% 21.5s\u001b[0K\u001b[1G95.5 MiB [] 0% 15.5s\u001b[0K\u001b[1G95.5 MiB [] 0% 10.0s\u001b[0K\u001b[1G95.5 MiB [] 1% 4.8s\u001b[0K\u001b[1G95.5 MiB [] 2% 3.6s\u001b[0K\u001b[1G95.5 MiB [] 3% 3.1s\u001b[0K\u001b[1G95.5 MiB [] 3% 2.8s\u001b[0K\u001b[1G95.5 MiB [] 4% 2.9s\u001b[0K\u001b[1G95.5 MiB [] 5% 2.7s\u001b[0K\u001b[1G95.5 MiB [] 6% 2.4s\u001b[0K\u001b[1G95.5 MiB [] 7% 2.1s\u001b[0K\u001b[1G95.5 MiB [] 9% 1.9s\u001b[0K\u001b[1G95.5 MiB [] 10% 1.8s\u001b[0K\u001b[1G95.5 MiB [] 11% 1.8s\u001b[0K\u001b[1G95.5 MiB [] 12% 1.7s\u001b[0K\u001b[1G95.5 MiB [] 14% 1.6s\u001b[0K\u001b[1G95.5 MiB [] 16% 1.5s\u001b[0K\u001b[1G95.5 MiB [] 17% 1.5s\u001b[0K\u001b[1G95.5 MiB [] 18% 1.4s\u001b[0K\u001b[1G95.5 MiB [] 19% 1.4s\u001b[0K\u001b[1G95.5 MiB [] 21% 1.3s\u001b[0K\u001b[1G95.5 MiB [] 23% 1.2s\u001b[0K\u001b[1G95.5 MiB [] 24% 1.2s\u001b[0K\u001b[1G95.5 MiB [] 25% 1.2s\u001b[0K\u001b[1G95.5 MiB [] 26% 1.1s\u001b[0K\u001b[1G95.5 MiB [] 28% 1.1s\u001b[0K\u001b[1G95.5 MiB [] 29% 1.1s\u001b[0K\u001b[1G95.5 MiB [] 30% 1.1s\u001b[0K\u001b[1G95.5 MiB [] 31% 1.0s\u001b[0K\u001b[1G95.5 MiB [] 32% 1.0s\u001b[0K\u001b[1G95.5 MiB [] 33% 1.0s\u001b[0K\u001b[1G95.5 MiB [] 35% 1.0s\u001b[0K\u001b[1G95.5 MiB [] 36% 0.9s\u001b[0K\u001b[1G95.5 MiB [] 37% 0.9s\u001b[0K\u001b[1G95.5 MiB [] 38% 0.9s\u001b[0K\u001b[1G95.5 MiB [] 39% 0.9s\u001b[0K\u001b[1G95.5 MiB [] 40% 0.9s\u001b[0K\u001b[1G95.5 MiB [] 41% 0.9s\u001b[0K\u001b[1G95.5 MiB [] 42% 0.9s\u001b[0K\u001b[1G95.5 MiB [] 44% 0.8s\u001b[0K\u001b[1G95.5 MiB [] 46% 0.8s\u001b[0K\u001b[1G95.5 MiB [] 47% 0.8s\u001b[0K\u001b[1G95.5 MiB [] 49% 0.7s\u001b[0K\u001b[1G95.5 MiB [] 50% 0.7s\u001b[0K\u001b[1G95.5 MiB [] 52% 0.7s\u001b[0K\u001b[1G95.5 MiB [] 53% 0.7s\u001b[0K\u001b[1G95.5 MiB [] 55% 0.6s\u001b[0K\u001b[1G95.5 MiB [] 56% 0.6s\u001b[0K\u001b[1G95.5 MiB [] 58% 0.6s\u001b[0K\u001b[1G95.5 MiB [] 60% 0.6s\u001b[0K\u001b[1G95.5 MiB [] 61% 0.5s\u001b[0K\u001b[1G95.5 MiB [] 62% 0.5s\u001b[0K\u001b[1G95.5 MiB [] 63% 0.5s\u001b[0K\u001b[1G95.5 MiB [] 64% 0.5s\u001b[0K\u001b[1G95.5 MiB [] 66% 0.5s\u001b[0K\u001b[1G95.5 MiB [] 68% 0.4s\u001b[0K\u001b[1G95.5 MiB [] 70% 0.4s\u001b[0K\u001b[1G95.5 MiB [] 72% 0.4s\u001b[0K\u001b[1G95.5 MiB [] 74% 0.3s\u001b[0K\u001b[1G95.5 MiB [] 75% 0.3s\u001b[0K\u001b[1G95.5 MiB [] 77% 0.3s\u001b[0K\u001b[1G95.5 MiB [] 79% 0.3s\u001b[0K\u001b[1G95.5 MiB [] 81% 0.2s\u001b[0K\u001b[1G95.5 MiB [] 82% 0.2s\u001b[0K\u001b[1G95.5 MiB [] 84% 0.2s\u001b[0K\u001b[1G95.5 MiB [] 86% 0.2s\u001b[0K\u001b[1G95.5 MiB [] 87% 0.2s\u001b[0K\u001b[1G95.5 MiB [] 89% 0.1s\u001b[0K\u001b[1G95.5 MiB [] 90% 0.1s\u001b[0K\u001b[1G95.5 MiB [] 91% 0.1s\u001b[0K\u001b[1G95.5 MiB [] 93% 0.1s\u001b[0K\u001b[1G95.5 MiB [] 95% 0.1s\u001b[0K\u001b[1G95.5 MiB [] 97% 0.0s\u001b[0K\u001b[1G95.5 MiB [] 98% 0.0s\u001b[0K\u001b[1G95.5 MiB [] 99% 0.0s\u001b[0K\u001b[1G95.5 MiB [] 100% 0.0s\u001b[0K\n",
            "Webkit 18.2 (playwright build v2104) downloaded to /root/.cache/ms-playwright/webkit-2104\n",
            "Downloading FFMPEG playwright build v1010\u001b[2m from https://playwright.azureedge.net/builds/ffmpeg/1010/ffmpeg-linux.zip\u001b[22m\n",
            "\u001b[1G2.3 MiB [] 0% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 2% 0.6s\u001b[0K\u001b[1G2.3 MiB [] 8% 0.3s\u001b[0K\u001b[1G2.3 MiB [] 19% 0.2s\u001b[0K\u001b[1G2.3 MiB [] 46% 0.1s\u001b[0K\u001b[1G2.3 MiB [] 88% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 100% 0.0s\u001b[0K\n",
            "FFMPEG playwright build v1010 downloaded to /root/.cache/ms-playwright/ffmpeg-1010\n",
            "Playwright Host validation warning: \n",
            "╔══════════════════════════════════════════════════════╗\n",
            "║ Host system is missing dependencies to run browsers. ║\n",
            "║ Missing libraries:                                   ║\n",
            "║     libwoff2dec.so.1.0.2                             ║\n",
            "║     libgstgl-1.0.so.0                                ║\n",
            "║     libgstcodecparsers-1.0.so.0                      ║\n",
            "║     libavif.so.13                                    ║\n",
            "║     libharfbuzz-icu.so.0                             ║\n",
            "║     libenchant-2.so.2                                ║\n",
            "║     libsecret-1.so.0                                 ║\n",
            "║     libhyphen.so.0                                   ║\n",
            "║     libmanette-0.2.so.0                              ║\n",
            "╚══════════════════════════════════════════════════════╝\n",
            "    at validateDependenciesLinux (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/dependencies.js:216:9)\n",
            "\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\u001b[39m\n",
            "    at async Registry._validateHostRequirements (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:753:43)\n",
            "    at async Registry._validateHostRequirementsForExecutableIfNeeded (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:851:7)\n",
            "    at async Registry.validateHostRequirementsForExecutablesIfNeeded (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:840:43)\n",
            "    at async t.<anonymous> (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/cli/program.js:137:7)\n"
          ]
        }
      ],
      "source": [
        "# Install all dependencies\n",
        "!pip install gensim\n",
        "!pip install nltk\n",
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install requests\n",
        "!pip install playwright\n",
        "!playwright install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpZnVCxZSQ8a",
        "outputId": "2fe5dcfb-4671-4155-a869-dd09368bc771"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_rus.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/maxent_treebank_pos_tagger_tab.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package punkt_tab to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data]    | Downloading package qc to /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package tagsets_json to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets_json.zip.\n",
            "[nltk_data]    | Downloading package timit to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2022 to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to\n",
            "[nltk_data]    |     /usr/local/lib/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Import libraries\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument,Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "from numpy.linalg import norm\n",
        "from termcolor import colored\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import re\n",
        "import plotly.graph_objects as go\n",
        "import nltk\n",
        "nltk.download('all', download_dir='/usr/local/lib/nltk_data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5CoVK6-iSWsU"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "jobs = pd.read_csv('/content/linkedin_tech_82k_git.csv')\n",
        "profiles=pd.read_csv('/content/Lab_project.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f57rqnCNPTyi"
      },
      "source": [
        "Keep only some columns to train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BtpytBBCPTyj"
      },
      "outputs": [],
      "source": [
        "jobs =jobs[['Employment type', 'Industries', 'Seniority level',\n",
        "       'company', 'description', 'education',\n",
        "       'location', 'months_experience', 'post_url', 'salary', 'title']]\n",
        "\n",
        "profiles=profiles[['about', 'certifications',\n",
        "       'current_company',\n",
        "       'education', 'experience','languages',\n",
        "       'position', 'url', 'volunteer_experience', 'сourses']]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UazPSTMlPTyj",
        "outputId": "abc54614-d4a8-4afe-a8c1-e7d0cd6b1f87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                data\n",
            "0  Full-time Broadcast Media Mid-Senior level Cyb...\n",
            "1  Full-time Hospital & Health Care, Medical Devi...\n",
            "2  Full-time Computer Hardware, Computer Software...\n",
            "3  Full-time Computer Hardware, Computer Software...\n",
            "4  Full-time Computer Hardware, Computer Software...\n"
          ]
        }
      ],
      "source": [
        "# Create a new column called 'data' and merge the values of the other columns into it\n",
        "jobs['data'] = jobs[['Employment type', 'Industries', 'Seniority level',\n",
        "       'company', 'description', 'education',\n",
        "       'location', 'title']].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1)\n",
        "# Drop the individual columns if you no longer need them\n",
        "jobs.drop(['Employment type', 'Industries', 'Seniority level',\n",
        "       'company', 'description', 'education',\n",
        "       'location', 'months_experience', 'post_url', 'salary', 'title'], axis=1, inplace=True)\n",
        "# Preview the updated dataframe\n",
        "print(jobs.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new column called 'data' and merge the values of the other columns into it\n",
        "profiles['data'] = profiles[['about', 'certifications',\n",
        "       'current_company',\n",
        "       'education', 'experience','languages',\n",
        "       'position', 'сourses']].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1)\n",
        "# Drop the individual columns if you no longer need them\n",
        "profiles.drop(['about', 'certifications',\n",
        "       'current_company',\n",
        "       'education', 'experience','languages',\n",
        "       'position', 'url', 'volunteer_experience', 'сourses'], axis=1, inplace=True)\n",
        "# Preview the updated dataframe\n",
        "print(profiles.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Csl_zlCozWc",
        "outputId": "f7a5b6d5-0ff6-46d5-816c-cd10cf65eeca"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                data\n",
            "0  [] {\"company_id\":null,\"industry\":\"Michigan Con...\n",
            "1  [] {\"company_id\":\"bcbsks\",\"industry\":\"Blue Cro...\n",
            "2  [] {\"company_id\":null,\"industry\":\"Holm Corruga...\n",
            "3  [{\"meta\":\"Issued Sep 2021 See credential\",\"sub...\n",
            "4  [] {\"company_id\":\"new-york-college-of-traditio...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_ntmC8RPTyj"
      },
      "source": [
        "#### 3. Tokenize data\n",
        "We tokenize the words in the 'data' column and tag them with unique identifiers using the TaggedDocument class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "sCAMBUD8Sorw"
      },
      "outputs": [],
      "source": [
        "jobs_data = list(jobs['data'])\n",
        "user_data=(list(profiles['data']))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Load NLTK stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_data(text):\n",
        "    \"\"\"\n",
        "    Tokenize and clean text by removing stopwords, punctuation, and non-alphabetic tokens.\n",
        "    \"\"\"\n",
        "    tokens = word_tokenize(text.lower())  # Tokenize and convert to lowercase\n",
        "    filtered_tokens = [\n",
        "        word for word in tokens\n",
        "        if word.isalpha() and word not in stop_words  # Keep only alphabetic words and non-stopwords\n",
        "    ]\n",
        "    return filtered_tokens\n"
      ],
      "metadata": {
        "id": "eJuLxLnfsvFK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Preprocess and tag profiles\n",
        "# tagged_profiles = [\n",
        "#     TaggedDocument(words=preprocess_data(profile), tags=[f\"profile_{i}\"])\n",
        "#     for i, profile in enumerate(user_data)\n",
        "# ]\n",
        "\n",
        "# # Preprocess and tag jobs\n",
        "# tagged_jobs = [\n",
        "#     TaggedDocument(words=preprocess_data(job), tags=[f\"job_{i}\"])\n",
        "#     for i, job in enumerate(jobs_data)\n",
        "# ]\n",
        "\n",
        "# # Combine both lists\n",
        "# tagged_data = tagged_profiles + tagged_jobs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "Sfcsm8jTdmMK",
        "outputId": "e5f4de52-e10a-46d0-9a6f-2d28b6148fdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-8f894372087d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Preprocess and tag jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m tagged_jobs = [\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mTaggedDocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"job_{i}\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjobs_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-8f894372087d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Preprocess and tag jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m tagged_jobs = [\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mTaggedDocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"job_{i}\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjobs_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m ]\n",
            "\u001b[0;32m<ipython-input-9-7b494f0f446b>\u001b[0m in \u001b[0;36mpreprocess_data\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mTokenize\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mclean\u001b[0m \u001b[0mtext\u001b[0m \u001b[0mby\u001b[0m \u001b[0mremoving\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpunctuation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnon\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0malphabetic\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \"\"\"\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Tokenize and convert to lowercase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     filtered_tokens = [\n\u001b[1;32m     14\u001b[0m         \u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \"\"\"\n\u001b[1;32m    142\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m     return [\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     ]\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     return [\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m     ]\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/destructive.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubstitution\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mENDING_QUOTES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubstitution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCONTRACTIONS2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Preprocess and tokenize profiles and jobs\n",
        "tokenized_profiles = [preprocess_data(profile) for profile in user_data]\n",
        "tokenized_jobs = [preprocess_data(job) for job in jobs_data]\n",
        "\n",
        "# Combine all tokenized data\n",
        "tokenized_data = tokenized_profiles + tokenized_jobs\n",
        "\n",
        "# Train Word2Vec model\n",
        "model = Word2Vec(sentences=tokenized_data, vector_size=100, min_count=2, epochs=10, workers=4)\n",
        "\n",
        "# Get the vocabulary keys\n",
        "keys = model.wv.key_to_index.keys()\n",
        "\n",
        "# Print the length of the vocabulary keys\n",
        "print(len(keys))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sn9VUH3Yhpmw",
        "outputId": "c3178d55-b012-4cb2-f831-19b053589e06"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19993\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pN_H6onBTamK"
      },
      "outputs": [],
      "source": [
        "# model = Word2Vec(vector_size=100, min_count=2, epochs=10, workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sv_Mtx4OWFgF"
      },
      "outputs": [],
      "source": [
        "# # Vocabulary building\n",
        "# model.build_vocab(tagged_data)\n",
        "# # Get the vocabulary keys\n",
        "# keys = model.wv.key_to_index.keys()\n",
        "# # Print the length of the vocabulary keys\n",
        "# print(len(keys))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBk5-LZLPTyk"
      },
      "source": [
        "#### 5. Train and save the model\n",
        "Train the model on tagged data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0jwx4eNAWYrI",
        "outputId": "2b6e9b90-d8db-4072-9aa9-336b58b40611",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 2/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 3/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 4/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 5/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 6/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 7/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 8/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 9/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 10/10\n",
            "Model saved\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "for epoch in range(model.epochs):\n",
        "    print(f\"Training epoch {epoch+1}/{model.epochs}\")\n",
        "# Train for all desired epochs in a single call\n",
        "    model.train(\n",
        "    tokenized_data,\n",
        "    total_examples=model.corpus_count,\n",
        "    epochs=10,  # Set the total number of training epochs\n",
        "    start_alpha=0.025,\n",
        "    end_alpha=0.0001\n",
        ")\n",
        "\n",
        "model.save('cv_job_maching.model')\n",
        "print(\"Model saved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFXCiT8GWgdP"
      },
      "source": [
        "#### 6. Inputs of CV and JD"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "profile_experiance= \"\"\"Company Name: Next Career\n",
        "Duration at company: 2 yrs 1 mo\n",
        "Job Name: Headhunter (via Next Career)\n",
        "Company: Flex\n",
        "Job Type: Contract\n",
        "Job Duration: Apr 2024 - May 2024 · 2 mos\n",
        "Job Location: Israel · Hybrid\n",
        "Additional Content: Recruitment as a Service\n",
        "Skills: N/A\n",
        "--------------------------------------------------\n",
        "Job Name: Headhunter (via Next Career)\n",
        "Company: Puzzle Projects LTD\n",
        "Job Type: Contract\n",
        "Job Duration: Feb 2024 - Apr 2024 · 3 mos\n",
        "Job Location: Israel · Remote\n",
        "Additional Content: Recruitment as a service for airspace and defence company.\n",
        "Skills: N/A\n",
        "--------------------------------------------------\n",
        "Job Name: Talent Acquisition Manager (via Next Career)\n",
        "Company: Discount Bank בנק דיסקונט\n",
        "Job Type: Contract\n",
        "Job Duration: 2023 - 2024 · 1 yr\n",
        "Job Location: Israel · Hybrid\n",
        "Additional Content: N/A\n",
        "Skills: N/A\n",
        "--------------------------------------------------\"\"\"\n",
        "\n",
        "profile_education=\"\"\"Education record:\n",
        "  Institution: University of HaifaUniversity of Haifa\n",
        "  Date: 2019 - 2021\n",
        "  Description: Master's degree, Organizational Behavior StudiesMaster's degree, Organizational Behavior Studies\n",
        "  Skills:\n",
        "  Additional Text: Grade: 94\n",
        "Education record:\n",
        "  Institution: University of HaifaUniversity of Haifa\n",
        "  Date: 2012 - 2014\n",
        "  Description: Bachelor's degree, Human Resources Management and ServicesBachelor's degree, Human Resources Management and Services\n",
        "  Skills:\n",
        "  Additional Text: Grade: 95\n",
        "Education record:\n",
        "  Institution: University of HaifaUniversity of Haifa\n",
        "  Date: 2012 - 2014\n",
        "  Description: Bachelor's degree, National Security Policy StudiesBachelor's degree, National Security Policy Studies\n",
        "  Skills:\n",
        "  Additional Text: \"\"\"\n",
        "\n",
        "profile_skills=\"\"\"['Analytical Skills', 'Candidate Management', 'Co-sourcing',\n",
        "   'Communication', 'Corporate Recruiting', 'Curriculum Vitae (CV)', 'English', 'Executive Search',\n",
        "    'Game Theory', 'Hiring', 'Human Resources (HR)', 'Interpersonal Skills', 'Interview Preparation',\n",
        "     'Leadership', 'LinkedIn', 'Microsoft Excel', 'Problem Solving', 'Recruiting',\n",
        "      'Recruitment-to-Recruitment', 'Resume Writing', 'Sales', 'Sourcing', 'Start-up Ventures',\n",
        "       'Statistical Data Analysis', 'Statistics', 'Strategic Sourcing', 'Strategy', 'Vacancies',\n",
        "        'Venture Capital']\"\"\""
      ],
      "metadata": {
        "id": "-yc-7RmxysmP"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(profile_skills)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRrxaPfyfpA4",
        "outputId": "f2edca62-7445-49cc-c28f-cffbfe092f8a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique skills: ['Analytical Skills', 'Candidate Management', 'Co-sourcing',\n",
            "   'Communication', 'Corporate Recruiting', 'Curriculum Vitae (CV)', 'English', 'Executive Search',\n",
            "    'Game Theory', 'Hiring', 'Human Resources (HR)', 'Interpersonal Skills', 'Interview Preparation',\n",
            "     'Leadership', 'LinkedIn', 'Microsoft Excel', 'Problem Solving', 'Recruiting',\n",
            "      'Recruitment-to-Recruitment', 'Resume Writing', 'Sales', 'Sourcing', 'Start-up Ventures',\n",
            "       'Statistical Data Analysis', 'Statistics', 'Strategic Sourcing', 'Strategy', 'Vacancies',\n",
            "        'Venture Capital']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from playwright.async_api import async_playwright\n",
        "import asyncio\n",
        "\n",
        "async def scrape_job_posting(job_url):\n",
        "    async with async_playwright() as p:\n",
        "        browser = await p.chromium.launch(headless=True)\n",
        "        context = await browser.new_context(user_agent=\"Mozilla/5.0\")\n",
        "        page = await context.new_page()\n",
        "\n",
        "        await page.goto(job_url)\n",
        "        await asyncio.sleep(5)  # Wait for 5 seconds\n",
        "\n",
        "        # Saving the page's HTML content for debugging purposes\n",
        "        html = await page.content()\n",
        "        with open(\"page_debug.html\", \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(html)\n",
        "        print(\"Saved page content to page_debug.html\")\n",
        "\n",
        "        # Taking a screenshot of the page for verification\n",
        "        await page.screenshot(path=\"screenshot.png\", full_page=True)\n",
        "        print(\"Screenshot saved as screenshot.png\")\n",
        "\n",
        "\n",
        "        # Extract job title\n",
        "        job_title = await page.inner_text('h1.top-card-layout__title')  # Adjust selector\n",
        "        company = await page.inner_text('a.topcard__org-name-link')  # Adjust selector\n",
        "        location = await page.inner_text('span.topcard__flavor--bullet')  # Adjust selector\n",
        "        description = await page.inner_text('div.show-more-less-html__markup')  # Adjust selector\n",
        "\n",
        "        print(f\"Job Title: {job_title}\")\n",
        "        print(f\"Company: {company}\")\n",
        "        print(f\"Location: {location}\")\n",
        "        print(f\"Description: {description}\")\n",
        "\n",
        "        await browser.close()\n",
        "        return job_title, company, location, description\n",
        "\n",
        "job_url = \"https://www.linkedin.com/jobs/view/4119256367\"\n",
        "job_title, company, location, description = await scrape_job_posting(job_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kZ2K0sdTDeB",
        "outputId": "26ea156d-c33c-4e4b-dd09-d1473c8e3c96"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved page content to page_debug.html\n",
            "Screenshot saved as screenshot.png\n",
            "Job Title: Recruitment Lead\n",
            "Company: Logistics company (NDA) \n",
            "Location:  Tel Aviv District, Israel\n",
            "Description: HR Operations (Recruitment Lead)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "We're seeking an HR Operations Manager in Tel Aviv.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "You will be responsible for:\n",
            "\n",
            "• Leading the recruitment team and overseeing mass recruitment for our business units\n",
            "\n",
            "• Managing new-hire orientation and onboarding\n",
            "\n",
            "Analyzing and optimizing business processes, advising on local law, and collaborating with providers\n",
            "\n",
            "• Implementing an effective onboarding system\n",
            "\n",
            "• Working with team metrics and KPIs\n",
            "\n",
            "• Developing a high-quality recruitment strategy and structure\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "You might be a fit if you:\n",
            "\n",
            "• Have solid relevant experience in talent development or recruitment\n",
            "\n",
            "• Have extensive knowledge of local employment law\n",
            "\n",
            "• Are a strong team player\n",
            "\n",
            "• Are experienced in supporting various HR projects and drafting or amending policies based on business needs\n",
            "\n",
            "• Are able to comfortably deal with ambiguity, interpret non-verbal communication, and understand cultural nuances\n",
            "\n",
            "• Are able to work independently and effectively in a fast-paced and changing environment\n",
            "\n",
            "• Possess excellent verbal and written communication skills\n",
            "\n",
            "• Are proficient in Hebrew and English\n",
            "\n",
            "• Have strong skills in Microsoft Office, especially Excel\n",
            "\n",
            "• Have experience in team management\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIElcDVCPTym"
      },
      "source": [
        "- **Develop a function to pre-process input text**:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# parts = [profile_experiance, profile_education, profile_skills]\n",
        "# profile_combined = \" \".join(parts)\n",
        "\n",
        "parts2 = [job_title, company, location, description]\n",
        "jobs_combined = \" \".join(parts2)\n",
        "\n"
      ],
      "metadata": {
        "id": "IvYbaOjP4WEM"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_job_description(job_description):\n",
        "    \"\"\"\n",
        "    Preprocess job description by removing excessive newlines and cleaning whitespace.\n",
        "    Args:\n",
        "        job_description (str): The raw job description text.\n",
        "    Returns:\n",
        "        str: The cleaned job description text.\n",
        "    \"\"\"\n",
        "    # Replace multiple newlines with a single space\n",
        "    cleaned_text = re.sub(r'\\s*\\n\\s*', ' ', job_description)\n",
        "\n",
        "    # Replace multiple spaces with a single space\n",
        "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
        "\n",
        "    # Strip leading and trailing whitespace\n",
        "    cleaned_text = cleaned_text.strip()\n",
        "\n",
        "    return cleaned_text\n"
      ],
      "metadata": {
        "id": "i86jYes1jAW8"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def preprocess_job_description(job_description):\n",
        "    \"\"\"\n",
        "    Preprocess job description by removing excessive newlines and cleaning whitespace.\n",
        "    \"\"\"\n",
        "    cleaned_text = re.sub(r'\\s*\\n\\s*', ' ', job_description)  # Remove newlines\n",
        "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)  # Remove extra spaces\n",
        "    return cleaned_text.strip()\n",
        "\n",
        "def extract_skills_precise(job_description):\n",
        "    \"\"\"\n",
        "    Extract skill-related sentences from a job description.\n",
        "    \"\"\"\n",
        "    # Preprocess the job description first\n",
        "    job_description = preprocess_job_description(job_description)\n",
        "\n",
        "    # Split the description into sentences\n",
        "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', job_description)\n",
        "\n",
        "    # Keywords to identify skill-related sentences\n",
        "    skill_keywords = [\n",
        "        \"hands-on experience\",\n",
        "        \"familiarity with\",\n",
        "        \"knowledge of\",\n",
        "        \"experience with\",\n",
        "        \"proficiency in\",\n",
        "        \"solid understanding of\",\n",
        "        \"ability to\",\n",
        "        \"required skills\",\n",
        "        \"technical skills\",\n",
        "        \"skills and qualifications\"\n",
        "    ]\n",
        "\n",
        "    # Filter sentences that contain any of the keywords\n",
        "    skill_sentences = [\n",
        "        sentence.strip() for sentence in sentences\n",
        "        if any(keyword in sentence.lower() for keyword in skill_keywords)\n",
        "    ]\n",
        "\n",
        "    # Refine further by ensuring the sentence has meaningful content\n",
        "    refined_sentences = [\n",
        "        sentence for sentence in skill_sentences\n",
        "        if len(sentence.split()) > 5  # Avoid very short or incomplete sentences\n",
        "    ]\n",
        "\n",
        "    return refined_sentences\n",
        "\n",
        "\n",
        "\n",
        "# Extract precise skill-related sentences\n",
        "job_skills_sentences = extract_skills_precise(description)\n",
        "\n",
        "print(\"Extracted Skill Sentences:\", job_skills_sentences)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04Hq6IWTS_8f",
        "outputId": "37f991a2-4428-43a7-8cc4-acbdfbd161b9"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Skill Sentences: ['You will be responsible for: • Leading the recruitment team and overseeing mass recruitment for our business units • Managing new-hire orientation and onboarding Analyzing and optimizing business processes, advising on local law, and collaborating with providers • Implementing an effective onboarding system • Working with team metrics and KPIs • Developing a high-quality recruitment strategy and structure You might be a fit if you: • Have solid relevant experience in talent development or recruitment • Have extensive knowledge of local employment law • Are a strong team player • Are experienced in supporting various HR projects and drafting or amending policies based on business needs • Are able to comfortably deal with ambiguity, interpret non-verbal communication, and understand cultural nuances • Are able to work independently and effectively in a fast-paced and changing environment • Possess excellent verbal and written communication skills • Are proficient in Hebrew and English • Have strong skills in Microsoft Office, especially Excel • Have experience in team management']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Extended stopwords for job descriptions\n",
        "stop_words = set(stopwords.words('english')).union({\n",
        "    'have', 'has','you' 'solid', 'relevant', 'extensive', 'proven', 'experience', 'possess', 'ability', 'skills'\n",
        "})\n",
        "\n",
        "def extract_relevant_phrases(sentence):\n",
        "    \"\"\"\n",
        "    Extract relevant phrases from a sentence by removing unnecessary words and focusing on key phrases.\n",
        "    \"\"\"\n",
        "    # Remove unnecessary phrases using regex\n",
        "    cleaned_sentence = re.sub(r'\\b(have|has|possess|proven|solid|relevant|extensive)\\b', '', sentence, flags=re.IGNORECASE)\n",
        "    cleaned_sentence = re.sub(r'\\b(experience (in|with|on|of|for))\\b', '', cleaned_sentence, flags=re.IGNORECASE)\n",
        "\n",
        "    # Tokenize and remove stopwords\n",
        "    tokens = word_tokenize(cleaned_sentence.lower())\n",
        "    tokens = [token for token in tokens if token not in stop_words and token.isalpha()]\n",
        "\n",
        "    # Extract noun phrases using POS tagging\n",
        "    pos_tags = pos_tag(tokens)\n",
        "    relevant_phrases = [word for word, tag in pos_tags if tag.startswith('NN')]  # Keep only nouns\n",
        "\n",
        "    return ' '.join(relevant_phrases)\n",
        "\n",
        "def process_job_description(description):\n",
        "    \"\"\"\n",
        "    Process job description to split into sentences and extract relevant parts.\n",
        "    \"\"\"\n",
        "    # Split the description into sentences\n",
        "    sentences = re.split(r'[\\n•]', description)\n",
        "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
        "\n",
        "    # Extract relevant phrases from each sentence\n",
        "    processed_sentences = [extract_relevant_phrases(sentence) for sentence in sentences]\n",
        "\n",
        "    # Remove empty results\n",
        "    return [phrase for phrase in processed_sentences if phrase.strip()]\n"
      ],
      "metadata": {
        "id": "fN1YK1XTGU_u"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Refined function to process job description\n",
        "def process_job_description(description):\n",
        "    \"\"\"\n",
        "    Process job description to split into sentences and extract relevant parts.\n",
        "    \"\"\"\n",
        "    # Split the description into sentences\n",
        "    sentences = re.split(r'[\\n•]', description)\n",
        "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
        "\n",
        "    # Extract relevant phrases from each sentence\n",
        "    processed_sentences = [extract_relevant_phrases(sentence) for sentence in sentences]\n",
        "\n",
        "    # Remove empty results\n",
        "    return [phrase for phrase in processed_sentences if phrase.strip()]\n",
        "\n",
        "# Refined phrase extraction function\n",
        "def extract_relevant_phrases(sentence):\n",
        "    \"\"\"\n",
        "    Extract relevant phrases from a sentence by removing unnecessary words and focusing on key phrases.\n",
        "    \"\"\"\n",
        "    # Remove unnecessary phrases using regex\n",
        "    cleaned_sentence = re.sub(r'\\b(have|has|possess|proven|solid|relevant|extensive)\\b', '', sentence, flags=re.IGNORECASE)\n",
        "    cleaned_sentence = re.sub(r'\\b(experience (in|with|on|of|for))\\b', '', cleaned_sentence, flags=re.IGNORECASE)\n",
        "\n",
        "    # Tokenize and remove stopwords\n",
        "    tokens = word_tokenize(cleaned_sentence.lower())\n",
        "    tokens = [token for token in tokens if token not in stop_words and token.isalpha()]\n",
        "\n",
        "    # Extract noun phrases using POS tagging\n",
        "    pos_tags = pos_tag(tokens)\n",
        "    relevant_phrases = [word for word, tag in pos_tags if tag.startswith('NN')]  # Keep only nouns\n",
        "\n",
        "    return ' '.join(relevant_phrases)\n",
        "\n",
        "# Updated Workflow\n",
        "\n",
        "# Process profile skills\n",
        "profile_skills_list = eval(profile_skills)  # Convert string to list\n",
        "profile_skills_processed = [preprocess(skill) for skill in profile_skills_list]\n",
        "\n",
        "# Process job description using refined function\n",
        "job_skills_processed = process_job_description(job_description)\n",
        "\n",
        "# Encode the processed skills\n",
        "profile_embeddings = model.encode(profile_skills_processed, convert_to_tensor=True)\n",
        "job_embeddings = model.encode(job_skills_processed, convert_to_tensor=True)\n",
        "\n",
        "# Compute cosine similarities\n",
        "cosine_similarities = util.pytorch_cos_sim(job_embeddings, profile_embeddings)\n",
        "\n",
        "# Adjust similarities based on threshold\n",
        "similarity_threshold = 0.5\n",
        "adjusted_similarities = []\n",
        "total_similarity = 0  # Track the total similarity\n",
        "comparison_count = 0  # Track the number of comparisons\n",
        "\n",
        "for i, job_skill in enumerate(job_skills_processed):\n",
        "    # Get all similarity scores for the current job skill\n",
        "    similarity_row = cosine_similarities[i].tolist()\n",
        "\n",
        "    # Check if any similarity exceeds the threshold\n",
        "    max_similarity = max(similarity_row)\n",
        "    if max_similarity > similarity_threshold:\n",
        "        # If above threshold, set all similarities in the row to the maximum\n",
        "        similarity_row = [1.0] * len(similarity_row)\n",
        "\n",
        "    # Add the adjusted row to the results\n",
        "    adjusted_similarities.append(similarity_row)\n",
        "\n",
        "    # Update total similarity and comparison count\n",
        "    total_similarity += sum(similarity_row)\n",
        "    comparison_count += len(similarity_row)\n",
        "\n",
        "# Calculate overall similarity as the average\n",
        "overall_similarity = total_similarity / comparison_count\n",
        "\n",
        "# Display adjusted results\n",
        "print(\"Adjusted Similarities:\")\n",
        "for i, job_skill in enumerate(job_skills_processed):\n",
        "    for j, profile_skill in enumerate(profile_skills_processed):\n",
        "        similarity_score = adjusted_similarities[i][j]\n",
        "        print(f\"Similarity between job skill '{job_skill}' and profile skill '{profile_skill}': {similarity_score:.2f}\")\n",
        "\n",
        "# Display overall similarity\n",
        "print(f\"\\nOverall Similarity: {overall_similarity:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoG25ADjjXTO",
        "outputId": "c54a59a3-c209-48cf-f4a2-a239461ae40f"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusted Similarities:\n",
            "Similarity between job skill 'hr operations lead' and profile skill 'analytical skills': 1.00\n",
            "Similarity between job skill 'hr operations lead' and profile skill 'candidate management': 1.00\n",
            "Similarity between job skill 'hr operations lead' and profile skill '': 1.00\n",
            "Similarity between job skill 'hr operations lead' and profile skill 'communication': 1.00\n",
            "Similarity between job skill 'hr operations lead' and profile skill 'corporate recruiting': 1.00\n",
            "Similarity between job skill 'hr operations lead' and profile skill 'curriculum vitae cv': 1.00\n",
            "Similarity between job skill 'hr operations lead' and profile skill 'english': 1.00\n",
            "Similarity between job skill 'hr operations lead' and profile skill 'executive search': 1.00\n",
            "Similarity between job skill 'hr operations lead' and profile skill 'game theory': 1.00\n",
            "Similarity between job skill 'hr operations lead' and profile skill 'hiring': 1.00\n",
            "Similarity between job skill 'hr operations lead' and profile skill 'human resources hr': 1.00\n",
            "Similarity between job skill 'hr operations lead' and profile skill 'interpersonal skills': 1.00\n",
            "Similarity between job skill 'hr operations lead' and profile skill 'interview preparation': 1.00\n",
            "Similarity between job skill 'hr operations lead' and profile skill 'leadership': 1.00\n",
            "Similarity between job skill 'hr operations lead' and profile skill 'linkedin': 1.00\n",
            "Similarity between job skill 'hr operations lead' and profile skill 'microsoft excel': 1.00\n",
            "Similarity between job skill 'hr operations lead' and profile skill 'problem solving': 1.00\n",
            "Similarity between job skill 'hr operations lead' and profile skill 'recruiting': 1.00\n",
            "Similarity between job skill 'hr operations lead' and profile skill '': 1.00\n",
            "Similarity between job skill 'hr operations lead' and profile skill 'resume writing': 1.00\n",
            "Similarity between job skill 'hr operations lead' and profile skill 'sales': 1.00\n",
            "Similarity between job skill 'hr operations lead' and profile skill 'sourcing': 1.00\n",
            "Similarity between job skill 'hr operations lead' and profile skill 'ventures': 1.00\n",
            "Similarity between job skill 'hr operations lead' and profile skill 'statistical data analysis': 1.00\n",
            "Similarity between job skill 'hr operations lead' and profile skill 'statistics': 1.00\n",
            "Similarity between job skill 'hr operations lead' and profile skill 'strategic sourcing': 1.00\n",
            "Similarity between job skill 'hr operations lead' and profile skill 'strategy': 1.00\n",
            "Similarity between job skill 'hr operations lead' and profile skill 'vacancies': 1.00\n",
            "Similarity between job skill 'hr operations lead' and profile skill 'venture capital': 1.00\n",
            "Similarity between job skill 'hr operations manager tel aviv' and profile skill 'analytical skills': 1.00\n",
            "Similarity between job skill 'hr operations manager tel aviv' and profile skill 'candidate management': 1.00\n",
            "Similarity between job skill 'hr operations manager tel aviv' and profile skill '': 1.00\n",
            "Similarity between job skill 'hr operations manager tel aviv' and profile skill 'communication': 1.00\n",
            "Similarity between job skill 'hr operations manager tel aviv' and profile skill 'corporate recruiting': 1.00\n",
            "Similarity between job skill 'hr operations manager tel aviv' and profile skill 'curriculum vitae cv': 1.00\n",
            "Similarity between job skill 'hr operations manager tel aviv' and profile skill 'english': 1.00\n",
            "Similarity between job skill 'hr operations manager tel aviv' and profile skill 'executive search': 1.00\n",
            "Similarity between job skill 'hr operations manager tel aviv' and profile skill 'game theory': 1.00\n",
            "Similarity between job skill 'hr operations manager tel aviv' and profile skill 'hiring': 1.00\n",
            "Similarity between job skill 'hr operations manager tel aviv' and profile skill 'human resources hr': 1.00\n",
            "Similarity between job skill 'hr operations manager tel aviv' and profile skill 'interpersonal skills': 1.00\n",
            "Similarity between job skill 'hr operations manager tel aviv' and profile skill 'interview preparation': 1.00\n",
            "Similarity between job skill 'hr operations manager tel aviv' and profile skill 'leadership': 1.00\n",
            "Similarity between job skill 'hr operations manager tel aviv' and profile skill 'linkedin': 1.00\n",
            "Similarity between job skill 'hr operations manager tel aviv' and profile skill 'microsoft excel': 1.00\n",
            "Similarity between job skill 'hr operations manager tel aviv' and profile skill 'problem solving': 1.00\n",
            "Similarity between job skill 'hr operations manager tel aviv' and profile skill 'recruiting': 1.00\n",
            "Similarity between job skill 'hr operations manager tel aviv' and profile skill '': 1.00\n",
            "Similarity between job skill 'hr operations manager tel aviv' and profile skill 'resume writing': 1.00\n",
            "Similarity between job skill 'hr operations manager tel aviv' and profile skill 'sales': 1.00\n",
            "Similarity between job skill 'hr operations manager tel aviv' and profile skill 'sourcing': 1.00\n",
            "Similarity between job skill 'hr operations manager tel aviv' and profile skill 'ventures': 1.00\n",
            "Similarity between job skill 'hr operations manager tel aviv' and profile skill 'statistical data analysis': 1.00\n",
            "Similarity between job skill 'hr operations manager tel aviv' and profile skill 'statistics': 1.00\n",
            "Similarity between job skill 'hr operations manager tel aviv' and profile skill 'strategic sourcing': 1.00\n",
            "Similarity between job skill 'hr operations manager tel aviv' and profile skill 'strategy': 1.00\n",
            "Similarity between job skill 'hr operations manager tel aviv' and profile skill 'vacancies': 1.00\n",
            "Similarity between job skill 'hr operations manager tel aviv' and profile skill 'venture capital': 1.00\n",
            "Similarity between job skill 'recruitment team mass recruitment business units' and profile skill 'analytical skills': 1.00\n",
            "Similarity between job skill 'recruitment team mass recruitment business units' and profile skill 'candidate management': 1.00\n",
            "Similarity between job skill 'recruitment team mass recruitment business units' and profile skill '': 1.00\n",
            "Similarity between job skill 'recruitment team mass recruitment business units' and profile skill 'communication': 1.00\n",
            "Similarity between job skill 'recruitment team mass recruitment business units' and profile skill 'corporate recruiting': 1.00\n",
            "Similarity between job skill 'recruitment team mass recruitment business units' and profile skill 'curriculum vitae cv': 1.00\n",
            "Similarity between job skill 'recruitment team mass recruitment business units' and profile skill 'english': 1.00\n",
            "Similarity between job skill 'recruitment team mass recruitment business units' and profile skill 'executive search': 1.00\n",
            "Similarity between job skill 'recruitment team mass recruitment business units' and profile skill 'game theory': 1.00\n",
            "Similarity between job skill 'recruitment team mass recruitment business units' and profile skill 'hiring': 1.00\n",
            "Similarity between job skill 'recruitment team mass recruitment business units' and profile skill 'human resources hr': 1.00\n",
            "Similarity between job skill 'recruitment team mass recruitment business units' and profile skill 'interpersonal skills': 1.00\n",
            "Similarity between job skill 'recruitment team mass recruitment business units' and profile skill 'interview preparation': 1.00\n",
            "Similarity between job skill 'recruitment team mass recruitment business units' and profile skill 'leadership': 1.00\n",
            "Similarity between job skill 'recruitment team mass recruitment business units' and profile skill 'linkedin': 1.00\n",
            "Similarity between job skill 'recruitment team mass recruitment business units' and profile skill 'microsoft excel': 1.00\n",
            "Similarity between job skill 'recruitment team mass recruitment business units' and profile skill 'problem solving': 1.00\n",
            "Similarity between job skill 'recruitment team mass recruitment business units' and profile skill 'recruiting': 1.00\n",
            "Similarity between job skill 'recruitment team mass recruitment business units' and profile skill '': 1.00\n",
            "Similarity between job skill 'recruitment team mass recruitment business units' and profile skill 'resume writing': 1.00\n",
            "Similarity between job skill 'recruitment team mass recruitment business units' and profile skill 'sales': 1.00\n",
            "Similarity between job skill 'recruitment team mass recruitment business units' and profile skill 'sourcing': 1.00\n",
            "Similarity between job skill 'recruitment team mass recruitment business units' and profile skill 'ventures': 1.00\n",
            "Similarity between job skill 'recruitment team mass recruitment business units' and profile skill 'statistical data analysis': 1.00\n",
            "Similarity between job skill 'recruitment team mass recruitment business units' and profile skill 'statistics': 1.00\n",
            "Similarity between job skill 'recruitment team mass recruitment business units' and profile skill 'strategic sourcing': 1.00\n",
            "Similarity between job skill 'recruitment team mass recruitment business units' and profile skill 'strategy': 1.00\n",
            "Similarity between job skill 'recruitment team mass recruitment business units' and profile skill 'vacancies': 1.00\n",
            "Similarity between job skill 'recruitment team mass recruitment business units' and profile skill 'venture capital': 1.00\n",
            "Similarity between job skill 'orientation' and profile skill 'analytical skills': 0.22\n",
            "Similarity between job skill 'orientation' and profile skill 'candidate management': 0.14\n",
            "Similarity between job skill 'orientation' and profile skill '': 0.22\n",
            "Similarity between job skill 'orientation' and profile skill 'communication': 0.16\n",
            "Similarity between job skill 'orientation' and profile skill 'corporate recruiting': 0.08\n",
            "Similarity between job skill 'orientation' and profile skill 'curriculum vitae cv': 0.08\n",
            "Similarity between job skill 'orientation' and profile skill 'english': 0.15\n",
            "Similarity between job skill 'orientation' and profile skill 'executive search': 0.17\n",
            "Similarity between job skill 'orientation' and profile skill 'game theory': 0.15\n",
            "Similarity between job skill 'orientation' and profile skill 'hiring': 0.15\n",
            "Similarity between job skill 'orientation' and profile skill 'human resources hr': 0.20\n",
            "Similarity between job skill 'orientation' and profile skill 'interpersonal skills': 0.22\n",
            "Similarity between job skill 'orientation' and profile skill 'interview preparation': 0.15\n",
            "Similarity between job skill 'orientation' and profile skill 'leadership': 0.28\n",
            "Similarity between job skill 'orientation' and profile skill 'linkedin': 0.06\n",
            "Similarity between job skill 'orientation' and profile skill 'microsoft excel': 0.13\n",
            "Similarity between job skill 'orientation' and profile skill 'problem solving': 0.17\n",
            "Similarity between job skill 'orientation' and profile skill 'recruiting': 0.13\n",
            "Similarity between job skill 'orientation' and profile skill '': 0.22\n",
            "Similarity between job skill 'orientation' and profile skill 'resume writing': 0.10\n",
            "Similarity between job skill 'orientation' and profile skill 'sales': 0.12\n",
            "Similarity between job skill 'orientation' and profile skill 'sourcing': 0.10\n",
            "Similarity between job skill 'orientation' and profile skill 'ventures': 0.12\n",
            "Similarity between job skill 'orientation' and profile skill 'statistical data analysis': 0.14\n",
            "Similarity between job skill 'orientation' and profile skill 'statistics': 0.17\n",
            "Similarity between job skill 'orientation' and profile skill 'strategic sourcing': 0.10\n",
            "Similarity between job skill 'orientation' and profile skill 'strategy': 0.27\n",
            "Similarity between job skill 'orientation' and profile skill 'vacancies': 0.12\n",
            "Similarity between job skill 'orientation' and profile skill 'venture capital': 0.11\n",
            "Similarity between job skill 'business processes law providers' and profile skill 'analytical skills': 0.10\n",
            "Similarity between job skill 'business processes law providers' and profile skill 'candidate management': 0.25\n",
            "Similarity between job skill 'business processes law providers' and profile skill '': 0.06\n",
            "Similarity between job skill 'business processes law providers' and profile skill 'communication': 0.10\n",
            "Similarity between job skill 'business processes law providers' and profile skill 'corporate recruiting': 0.18\n",
            "Similarity between job skill 'business processes law providers' and profile skill 'curriculum vitae cv': 0.10\n",
            "Similarity between job skill 'business processes law providers' and profile skill 'english': 0.14\n",
            "Similarity between job skill 'business processes law providers' and profile skill 'executive search': 0.21\n",
            "Similarity between job skill 'business processes law providers' and profile skill 'game theory': 0.07\n",
            "Similarity between job skill 'business processes law providers' and profile skill 'hiring': 0.23\n",
            "Similarity between job skill 'business processes law providers' and profile skill 'human resources hr': 0.20\n",
            "Similarity between job skill 'business processes law providers' and profile skill 'interpersonal skills': 0.16\n",
            "Similarity between job skill 'business processes law providers' and profile skill 'interview preparation': 0.09\n",
            "Similarity between job skill 'business processes law providers' and profile skill 'leadership': 0.18\n",
            "Similarity between job skill 'business processes law providers' and profile skill 'linkedin': 0.11\n",
            "Similarity between job skill 'business processes law providers' and profile skill 'microsoft excel': 0.13\n",
            "Similarity between job skill 'business processes law providers' and profile skill 'problem solving': 0.09\n",
            "Similarity between job skill 'business processes law providers' and profile skill 'recruiting': 0.09\n",
            "Similarity between job skill 'business processes law providers' and profile skill '': 0.06\n",
            "Similarity between job skill 'business processes law providers' and profile skill 'resume writing': 0.09\n",
            "Similarity between job skill 'business processes law providers' and profile skill 'sales': 0.26\n",
            "Similarity between job skill 'business processes law providers' and profile skill 'sourcing': 0.30\n",
            "Similarity between job skill 'business processes law providers' and profile skill 'ventures': 0.26\n",
            "Similarity between job skill 'business processes law providers' and profile skill 'statistical data analysis': 0.11\n",
            "Similarity between job skill 'business processes law providers' and profile skill 'statistics': 0.09\n",
            "Similarity between job skill 'business processes law providers' and profile skill 'strategic sourcing': 0.28\n",
            "Similarity between job skill 'business processes law providers' and profile skill 'strategy': 0.06\n",
            "Similarity between job skill 'business processes law providers' and profile skill 'vacancies': 0.17\n",
            "Similarity between job skill 'business processes law providers' and profile skill 'venture capital': 0.25\n",
            "Similarity between job skill 'onboarding system' and profile skill 'analytical skills': 0.11\n",
            "Similarity between job skill 'onboarding system' and profile skill 'candidate management': 0.20\n",
            "Similarity between job skill 'onboarding system' and profile skill '': 0.11\n",
            "Similarity between job skill 'onboarding system' and profile skill 'communication': 0.08\n",
            "Similarity between job skill 'onboarding system' and profile skill 'corporate recruiting': 0.29\n",
            "Similarity between job skill 'onboarding system' and profile skill 'curriculum vitae cv': 0.18\n",
            "Similarity between job skill 'onboarding system' and profile skill 'english': 0.04\n",
            "Similarity between job skill 'onboarding system' and profile skill 'executive search': 0.14\n",
            "Similarity between job skill 'onboarding system' and profile skill 'game theory': 0.07\n",
            "Similarity between job skill 'onboarding system' and profile skill 'hiring': 0.19\n",
            "Similarity between job skill 'onboarding system' and profile skill 'human resources hr': 0.10\n",
            "Similarity between job skill 'onboarding system' and profile skill 'interpersonal skills': 0.08\n",
            "Similarity between job skill 'onboarding system' and profile skill 'interview preparation': 0.21\n",
            "Similarity between job skill 'onboarding system' and profile skill 'leadership': 0.12\n",
            "Similarity between job skill 'onboarding system' and profile skill 'linkedin': 0.09\n",
            "Similarity between job skill 'onboarding system' and profile skill 'microsoft excel': 0.17\n",
            "Similarity between job skill 'onboarding system' and profile skill 'problem solving': 0.16\n",
            "Similarity between job skill 'onboarding system' and profile skill 'recruiting': 0.29\n",
            "Similarity between job skill 'onboarding system' and profile skill '': 0.11\n",
            "Similarity between job skill 'onboarding system' and profile skill 'resume writing': 0.13\n",
            "Similarity between job skill 'onboarding system' and profile skill 'sales': 0.13\n",
            "Similarity between job skill 'onboarding system' and profile skill 'sourcing': 0.14\n",
            "Similarity between job skill 'onboarding system' and profile skill 'ventures': 0.15\n",
            "Similarity between job skill 'onboarding system' and profile skill 'statistical data analysis': 0.21\n",
            "Similarity between job skill 'onboarding system' and profile skill 'statistics': 0.09\n",
            "Similarity between job skill 'onboarding system' and profile skill 'strategic sourcing': 0.17\n",
            "Similarity between job skill 'onboarding system' and profile skill 'strategy': 0.11\n",
            "Similarity between job skill 'onboarding system' and profile skill 'vacancies': 0.09\n",
            "Similarity between job skill 'onboarding system' and profile skill 'venture capital': 0.18\n",
            "Similarity between job skill 'team metrics' and profile skill 'analytical skills': 0.19\n",
            "Similarity between job skill 'team metrics' and profile skill 'candidate management': 0.25\n",
            "Similarity between job skill 'team metrics' and profile skill '': 0.13\n",
            "Similarity between job skill 'team metrics' and profile skill 'communication': 0.25\n",
            "Similarity between job skill 'team metrics' and profile skill 'corporate recruiting': 0.32\n",
            "Similarity between job skill 'team metrics' and profile skill 'curriculum vitae cv': 0.04\n",
            "Similarity between job skill 'team metrics' and profile skill 'english': 0.23\n",
            "Similarity between job skill 'team metrics' and profile skill 'executive search': 0.11\n",
            "Similarity between job skill 'team metrics' and profile skill 'game theory': 0.26\n",
            "Similarity between job skill 'team metrics' and profile skill 'hiring': 0.13\n",
            "Similarity between job skill 'team metrics' and profile skill 'human resources hr': 0.24\n",
            "Similarity between job skill 'team metrics' and profile skill 'interpersonal skills': 0.22\n",
            "Similarity between job skill 'team metrics' and profile skill 'interview preparation': 0.15\n",
            "Similarity between job skill 'team metrics' and profile skill 'leadership': 0.32\n",
            "Similarity between job skill 'team metrics' and profile skill 'linkedin': 0.15\n",
            "Similarity between job skill 'team metrics' and profile skill 'microsoft excel': 0.20\n",
            "Similarity between job skill 'team metrics' and profile skill 'problem solving': 0.18\n",
            "Similarity between job skill 'team metrics' and profile skill 'recruiting': 0.34\n",
            "Similarity between job skill 'team metrics' and profile skill '': 0.13\n",
            "Similarity between job skill 'team metrics' and profile skill 'resume writing': 0.06\n",
            "Similarity between job skill 'team metrics' and profile skill 'sales': 0.23\n",
            "Similarity between job skill 'team metrics' and profile skill 'sourcing': 0.25\n",
            "Similarity between job skill 'team metrics' and profile skill 'ventures': 0.16\n",
            "Similarity between job skill 'team metrics' and profile skill 'statistical data analysis': 0.36\n",
            "Similarity between job skill 'team metrics' and profile skill 'statistics': 0.40\n",
            "Similarity between job skill 'team metrics' and profile skill 'strategic sourcing': 0.32\n",
            "Similarity between job skill 'team metrics' and profile skill 'strategy': 0.29\n",
            "Similarity between job skill 'team metrics' and profile skill 'vacancies': 0.14\n",
            "Similarity between job skill 'team metrics' and profile skill 'venture capital': 0.13\n",
            "Similarity between job skill 'strategy structure' and profile skill 'analytical skills': 1.00\n",
            "Similarity between job skill 'strategy structure' and profile skill 'candidate management': 1.00\n",
            "Similarity between job skill 'strategy structure' and profile skill '': 1.00\n",
            "Similarity between job skill 'strategy structure' and profile skill 'communication': 1.00\n",
            "Similarity between job skill 'strategy structure' and profile skill 'corporate recruiting': 1.00\n",
            "Similarity between job skill 'strategy structure' and profile skill 'curriculum vitae cv': 1.00\n",
            "Similarity between job skill 'strategy structure' and profile skill 'english': 1.00\n",
            "Similarity between job skill 'strategy structure' and profile skill 'executive search': 1.00\n",
            "Similarity between job skill 'strategy structure' and profile skill 'game theory': 1.00\n",
            "Similarity between job skill 'strategy structure' and profile skill 'hiring': 1.00\n",
            "Similarity between job skill 'strategy structure' and profile skill 'human resources hr': 1.00\n",
            "Similarity between job skill 'strategy structure' and profile skill 'interpersonal skills': 1.00\n",
            "Similarity between job skill 'strategy structure' and profile skill 'interview preparation': 1.00\n",
            "Similarity between job skill 'strategy structure' and profile skill 'leadership': 1.00\n",
            "Similarity between job skill 'strategy structure' and profile skill 'linkedin': 1.00\n",
            "Similarity between job skill 'strategy structure' and profile skill 'microsoft excel': 1.00\n",
            "Similarity between job skill 'strategy structure' and profile skill 'problem solving': 1.00\n",
            "Similarity between job skill 'strategy structure' and profile skill 'recruiting': 1.00\n",
            "Similarity between job skill 'strategy structure' and profile skill '': 1.00\n",
            "Similarity between job skill 'strategy structure' and profile skill 'resume writing': 1.00\n",
            "Similarity between job skill 'strategy structure' and profile skill 'sales': 1.00\n",
            "Similarity between job skill 'strategy structure' and profile skill 'sourcing': 1.00\n",
            "Similarity between job skill 'strategy structure' and profile skill 'ventures': 1.00\n",
            "Similarity between job skill 'strategy structure' and profile skill 'statistical data analysis': 1.00\n",
            "Similarity between job skill 'strategy structure' and profile skill 'statistics': 1.00\n",
            "Similarity between job skill 'strategy structure' and profile skill 'strategic sourcing': 1.00\n",
            "Similarity between job skill 'strategy structure' and profile skill 'strategy': 1.00\n",
            "Similarity between job skill 'strategy structure' and profile skill 'vacancies': 1.00\n",
            "Similarity between job skill 'strategy structure' and profile skill 'venture capital': 1.00\n",
            "Similarity between job skill 'talent development recruitment' and profile skill 'analytical skills': 1.00\n",
            "Similarity between job skill 'talent development recruitment' and profile skill 'candidate management': 1.00\n",
            "Similarity between job skill 'talent development recruitment' and profile skill '': 1.00\n",
            "Similarity between job skill 'talent development recruitment' and profile skill 'communication': 1.00\n",
            "Similarity between job skill 'talent development recruitment' and profile skill 'corporate recruiting': 1.00\n",
            "Similarity between job skill 'talent development recruitment' and profile skill 'curriculum vitae cv': 1.00\n",
            "Similarity between job skill 'talent development recruitment' and profile skill 'english': 1.00\n",
            "Similarity between job skill 'talent development recruitment' and profile skill 'executive search': 1.00\n",
            "Similarity between job skill 'talent development recruitment' and profile skill 'game theory': 1.00\n",
            "Similarity between job skill 'talent development recruitment' and profile skill 'hiring': 1.00\n",
            "Similarity between job skill 'talent development recruitment' and profile skill 'human resources hr': 1.00\n",
            "Similarity between job skill 'talent development recruitment' and profile skill 'interpersonal skills': 1.00\n",
            "Similarity between job skill 'talent development recruitment' and profile skill 'interview preparation': 1.00\n",
            "Similarity between job skill 'talent development recruitment' and profile skill 'leadership': 1.00\n",
            "Similarity between job skill 'talent development recruitment' and profile skill 'linkedin': 1.00\n",
            "Similarity between job skill 'talent development recruitment' and profile skill 'microsoft excel': 1.00\n",
            "Similarity between job skill 'talent development recruitment' and profile skill 'problem solving': 1.00\n",
            "Similarity between job skill 'talent development recruitment' and profile skill 'recruiting': 1.00\n",
            "Similarity between job skill 'talent development recruitment' and profile skill '': 1.00\n",
            "Similarity between job skill 'talent development recruitment' and profile skill 'resume writing': 1.00\n",
            "Similarity between job skill 'talent development recruitment' and profile skill 'sales': 1.00\n",
            "Similarity between job skill 'talent development recruitment' and profile skill 'sourcing': 1.00\n",
            "Similarity between job skill 'talent development recruitment' and profile skill 'ventures': 1.00\n",
            "Similarity between job skill 'talent development recruitment' and profile skill 'statistical data analysis': 1.00\n",
            "Similarity between job skill 'talent development recruitment' and profile skill 'statistics': 1.00\n",
            "Similarity between job skill 'talent development recruitment' and profile skill 'strategic sourcing': 1.00\n",
            "Similarity between job skill 'talent development recruitment' and profile skill 'strategy': 1.00\n",
            "Similarity between job skill 'talent development recruitment' and profile skill 'vacancies': 1.00\n",
            "Similarity between job skill 'talent development recruitment' and profile skill 'venture capital': 1.00\n",
            "Similarity between job skill 'employment law' and profile skill 'analytical skills': 0.06\n",
            "Similarity between job skill 'employment law' and profile skill 'candidate management': 0.26\n",
            "Similarity between job skill 'employment law' and profile skill '': 0.09\n",
            "Similarity between job skill 'employment law' and profile skill 'communication': 0.13\n",
            "Similarity between job skill 'employment law' and profile skill 'corporate recruiting': 0.29\n",
            "Similarity between job skill 'employment law' and profile skill 'curriculum vitae cv': 0.08\n",
            "Similarity between job skill 'employment law' and profile skill 'english': 0.20\n",
            "Similarity between job skill 'employment law' and profile skill 'executive search': 0.19\n",
            "Similarity between job skill 'employment law' and profile skill 'game theory': 0.10\n",
            "Similarity between job skill 'employment law' and profile skill 'hiring': 0.48\n",
            "Similarity between job skill 'employment law' and profile skill 'human resources hr': 0.34\n",
            "Similarity between job skill 'employment law' and profile skill 'interpersonal skills': 0.17\n",
            "Similarity between job skill 'employment law' and profile skill 'interview preparation': 0.17\n",
            "Similarity between job skill 'employment law' and profile skill 'leadership': 0.07\n",
            "Similarity between job skill 'employment law' and profile skill 'linkedin': 0.12\n",
            "Similarity between job skill 'employment law' and profile skill 'microsoft excel': 0.11\n",
            "Similarity between job skill 'employment law' and profile skill 'problem solving': 0.10\n",
            "Similarity between job skill 'employment law' and profile skill 'recruiting': 0.27\n",
            "Similarity between job skill 'employment law' and profile skill '': 0.09\n",
            "Similarity between job skill 'employment law' and profile skill 'resume writing': 0.28\n",
            "Similarity between job skill 'employment law' and profile skill 'sales': 0.22\n",
            "Similarity between job skill 'employment law' and profile skill 'sourcing': 0.26\n",
            "Similarity between job skill 'employment law' and profile skill 'ventures': 0.19\n",
            "Similarity between job skill 'employment law' and profile skill 'statistical data analysis': 0.09\n",
            "Similarity between job skill 'employment law' and profile skill 'statistics': 0.16\n",
            "Similarity between job skill 'employment law' and profile skill 'strategic sourcing': 0.13\n",
            "Similarity between job skill 'employment law' and profile skill 'strategy': 0.07\n",
            "Similarity between job skill 'employment law' and profile skill 'vacancies': 0.26\n",
            "Similarity between job skill 'employment law' and profile skill 'venture capital': 0.20\n",
            "Similarity between job skill 'team player' and profile skill 'analytical skills': 0.19\n",
            "Similarity between job skill 'team player' and profile skill 'candidate management': 0.32\n",
            "Similarity between job skill 'team player' and profile skill '': 0.28\n",
            "Similarity between job skill 'team player' and profile skill 'communication': 0.28\n",
            "Similarity between job skill 'team player' and profile skill 'corporate recruiting': 0.22\n",
            "Similarity between job skill 'team player' and profile skill 'curriculum vitae cv': 0.03\n",
            "Similarity between job skill 'team player' and profile skill 'english': 0.32\n",
            "Similarity between job skill 'team player' and profile skill 'executive search': 0.12\n",
            "Similarity between job skill 'team player' and profile skill 'game theory': 0.33\n",
            "Similarity between job skill 'team player' and profile skill 'hiring': 0.22\n",
            "Similarity between job skill 'team player' and profile skill 'human resources hr': 0.14\n",
            "Similarity between job skill 'team player' and profile skill 'interpersonal skills': 0.28\n",
            "Similarity between job skill 'team player' and profile skill 'interview preparation': 0.12\n",
            "Similarity between job skill 'team player' and profile skill 'leadership': 0.45\n",
            "Similarity between job skill 'team player' and profile skill 'linkedin': 0.09\n",
            "Similarity between job skill 'team player' and profile skill 'microsoft excel': 0.18\n",
            "Similarity between job skill 'team player' and profile skill 'problem solving': 0.25\n",
            "Similarity between job skill 'team player' and profile skill 'recruiting': 0.34\n",
            "Similarity between job skill 'team player' and profile skill '': 0.28\n",
            "Similarity between job skill 'team player' and profile skill 'resume writing': 0.02\n",
            "Similarity between job skill 'team player' and profile skill 'sales': 0.22\n",
            "Similarity between job skill 'team player' and profile skill 'sourcing': 0.20\n",
            "Similarity between job skill 'team player' and profile skill 'ventures': 0.19\n",
            "Similarity between job skill 'team player' and profile skill 'statistical data analysis': 0.17\n",
            "Similarity between job skill 'team player' and profile skill 'statistics': 0.16\n",
            "Similarity between job skill 'team player' and profile skill 'strategic sourcing': 0.21\n",
            "Similarity between job skill 'team player' and profile skill 'strategy': 0.29\n",
            "Similarity between job skill 'team player' and profile skill 'vacancies': 0.10\n",
            "Similarity between job skill 'team player' and profile skill 'venture capital': 0.12\n",
            "Similarity between job skill 'projects policies business needs' and profile skill 'analytical skills': 0.12\n",
            "Similarity between job skill 'projects policies business needs' and profile skill 'candidate management': 0.24\n",
            "Similarity between job skill 'projects policies business needs' and profile skill '': 0.06\n",
            "Similarity between job skill 'projects policies business needs' and profile skill 'communication': 0.11\n",
            "Similarity between job skill 'projects policies business needs' and profile skill 'corporate recruiting': 0.21\n",
            "Similarity between job skill 'projects policies business needs' and profile skill 'curriculum vitae cv': 0.08\n",
            "Similarity between job skill 'projects policies business needs' and profile skill 'english': 0.08\n",
            "Similarity between job skill 'projects policies business needs' and profile skill 'executive search': 0.18\n",
            "Similarity between job skill 'projects policies business needs' and profile skill 'game theory': 0.09\n",
            "Similarity between job skill 'projects policies business needs' and profile skill 'hiring': 0.15\n",
            "Similarity between job skill 'projects policies business needs' and profile skill 'human resources hr': 0.14\n",
            "Similarity between job skill 'projects policies business needs' and profile skill 'interpersonal skills': 0.09\n",
            "Similarity between job skill 'projects policies business needs' and profile skill 'interview preparation': 0.11\n",
            "Similarity between job skill 'projects policies business needs' and profile skill 'leadership': 0.17\n",
            "Similarity between job skill 'projects policies business needs' and profile skill 'linkedin': 0.06\n",
            "Similarity between job skill 'projects policies business needs' and profile skill 'microsoft excel': 0.13\n",
            "Similarity between job skill 'projects policies business needs' and profile skill 'problem solving': 0.22\n",
            "Similarity between job skill 'projects policies business needs' and profile skill 'recruiting': 0.09\n",
            "Similarity between job skill 'projects policies business needs' and profile skill '': 0.06\n",
            "Similarity between job skill 'projects policies business needs' and profile skill 'resume writing': 0.07\n",
            "Similarity between job skill 'projects policies business needs' and profile skill 'sales': 0.12\n",
            "Similarity between job skill 'projects policies business needs' and profile skill 'sourcing': 0.26\n",
            "Similarity between job skill 'projects policies business needs' and profile skill 'ventures': 0.27\n",
            "Similarity between job skill 'projects policies business needs' and profile skill 'statistical data analysis': 0.12\n",
            "Similarity between job skill 'projects policies business needs' and profile skill 'statistics': 0.13\n",
            "Similarity between job skill 'projects policies business needs' and profile skill 'strategic sourcing': 0.35\n",
            "Similarity between job skill 'projects policies business needs' and profile skill 'strategy': 0.22\n",
            "Similarity between job skill 'projects policies business needs' and profile skill 'vacancies': 0.11\n",
            "Similarity between job skill 'projects policies business needs' and profile skill 'venture capital': 0.23\n",
            "Similarity between job skill 'ambiguity communication nuances' and profile skill 'analytical skills': 1.00\n",
            "Similarity between job skill 'ambiguity communication nuances' and profile skill 'candidate management': 1.00\n",
            "Similarity between job skill 'ambiguity communication nuances' and profile skill '': 1.00\n",
            "Similarity between job skill 'ambiguity communication nuances' and profile skill 'communication': 1.00\n",
            "Similarity between job skill 'ambiguity communication nuances' and profile skill 'corporate recruiting': 1.00\n",
            "Similarity between job skill 'ambiguity communication nuances' and profile skill 'curriculum vitae cv': 1.00\n",
            "Similarity between job skill 'ambiguity communication nuances' and profile skill 'english': 1.00\n",
            "Similarity between job skill 'ambiguity communication nuances' and profile skill 'executive search': 1.00\n",
            "Similarity between job skill 'ambiguity communication nuances' and profile skill 'game theory': 1.00\n",
            "Similarity between job skill 'ambiguity communication nuances' and profile skill 'hiring': 1.00\n",
            "Similarity between job skill 'ambiguity communication nuances' and profile skill 'human resources hr': 1.00\n",
            "Similarity between job skill 'ambiguity communication nuances' and profile skill 'interpersonal skills': 1.00\n",
            "Similarity between job skill 'ambiguity communication nuances' and profile skill 'interview preparation': 1.00\n",
            "Similarity between job skill 'ambiguity communication nuances' and profile skill 'leadership': 1.00\n",
            "Similarity between job skill 'ambiguity communication nuances' and profile skill 'linkedin': 1.00\n",
            "Similarity between job skill 'ambiguity communication nuances' and profile skill 'microsoft excel': 1.00\n",
            "Similarity between job skill 'ambiguity communication nuances' and profile skill 'problem solving': 1.00\n",
            "Similarity between job skill 'ambiguity communication nuances' and profile skill 'recruiting': 1.00\n",
            "Similarity between job skill 'ambiguity communication nuances' and profile skill '': 1.00\n",
            "Similarity between job skill 'ambiguity communication nuances' and profile skill 'resume writing': 1.00\n",
            "Similarity between job skill 'ambiguity communication nuances' and profile skill 'sales': 1.00\n",
            "Similarity between job skill 'ambiguity communication nuances' and profile skill 'sourcing': 1.00\n",
            "Similarity between job skill 'ambiguity communication nuances' and profile skill 'ventures': 1.00\n",
            "Similarity between job skill 'ambiguity communication nuances' and profile skill 'statistical data analysis': 1.00\n",
            "Similarity between job skill 'ambiguity communication nuances' and profile skill 'statistics': 1.00\n",
            "Similarity between job skill 'ambiguity communication nuances' and profile skill 'strategic sourcing': 1.00\n",
            "Similarity between job skill 'ambiguity communication nuances' and profile skill 'strategy': 1.00\n",
            "Similarity between job skill 'ambiguity communication nuances' and profile skill 'vacancies': 1.00\n",
            "Similarity between job skill 'ambiguity communication nuances' and profile skill 'venture capital': 1.00\n",
            "Similarity between job skill 'work environment' and profile skill 'analytical skills': 0.29\n",
            "Similarity between job skill 'work environment' and profile skill 'candidate management': 0.29\n",
            "Similarity between job skill 'work environment' and profile skill '': 0.14\n",
            "Similarity between job skill 'work environment' and profile skill 'communication': 0.28\n",
            "Similarity between job skill 'work environment' and profile skill 'corporate recruiting': 0.35\n",
            "Similarity between job skill 'work environment' and profile skill 'curriculum vitae cv': 0.12\n",
            "Similarity between job skill 'work environment' and profile skill 'english': 0.27\n",
            "Similarity between job skill 'work environment' and profile skill 'executive search': 0.30\n",
            "Similarity between job skill 'work environment' and profile skill 'game theory': 0.16\n",
            "Similarity between job skill 'work environment' and profile skill 'hiring': 0.45\n",
            "Similarity between job skill 'work environment' and profile skill 'human resources hr': 0.47\n",
            "Similarity between job skill 'work environment' and profile skill 'interpersonal skills': 0.34\n",
            "Similarity between job skill 'work environment' and profile skill 'interview preparation': 0.31\n",
            "Similarity between job skill 'work environment' and profile skill 'leadership': 0.29\n",
            "Similarity between job skill 'work environment' and profile skill 'linkedin': 0.13\n",
            "Similarity between job skill 'work environment' and profile skill 'microsoft excel': 0.14\n",
            "Similarity between job skill 'work environment' and profile skill 'problem solving': 0.29\n",
            "Similarity between job skill 'work environment' and profile skill 'recruiting': 0.31\n",
            "Similarity between job skill 'work environment' and profile skill '': 0.14\n",
            "Similarity between job skill 'work environment' and profile skill 'resume writing': 0.23\n",
            "Similarity between job skill 'work environment' and profile skill 'sales': 0.23\n",
            "Similarity between job skill 'work environment' and profile skill 'sourcing': 0.35\n",
            "Similarity between job skill 'work environment' and profile skill 'ventures': 0.28\n",
            "Similarity between job skill 'work environment' and profile skill 'statistical data analysis': 0.18\n",
            "Similarity between job skill 'work environment' and profile skill 'statistics': 0.19\n",
            "Similarity between job skill 'work environment' and profile skill 'strategic sourcing': 0.28\n",
            "Similarity between job skill 'work environment' and profile skill 'strategy': 0.20\n",
            "Similarity between job skill 'work environment' and profile skill 'vacancies': 0.31\n",
            "Similarity between job skill 'work environment' and profile skill 'venture capital': 0.19\n",
            "Similarity between job skill 'communication' and profile skill 'analytical skills': 1.00\n",
            "Similarity between job skill 'communication' and profile skill 'candidate management': 1.00\n",
            "Similarity between job skill 'communication' and profile skill '': 1.00\n",
            "Similarity between job skill 'communication' and profile skill 'communication': 1.00\n",
            "Similarity between job skill 'communication' and profile skill 'corporate recruiting': 1.00\n",
            "Similarity between job skill 'communication' and profile skill 'curriculum vitae cv': 1.00\n",
            "Similarity between job skill 'communication' and profile skill 'english': 1.00\n",
            "Similarity between job skill 'communication' and profile skill 'executive search': 1.00\n",
            "Similarity between job skill 'communication' and profile skill 'game theory': 1.00\n",
            "Similarity between job skill 'communication' and profile skill 'hiring': 1.00\n",
            "Similarity between job skill 'communication' and profile skill 'human resources hr': 1.00\n",
            "Similarity between job skill 'communication' and profile skill 'interpersonal skills': 1.00\n",
            "Similarity between job skill 'communication' and profile skill 'interview preparation': 1.00\n",
            "Similarity between job skill 'communication' and profile skill 'leadership': 1.00\n",
            "Similarity between job skill 'communication' and profile skill 'linkedin': 1.00\n",
            "Similarity between job skill 'communication' and profile skill 'microsoft excel': 1.00\n",
            "Similarity between job skill 'communication' and profile skill 'problem solving': 1.00\n",
            "Similarity between job skill 'communication' and profile skill 'recruiting': 1.00\n",
            "Similarity between job skill 'communication' and profile skill '': 1.00\n",
            "Similarity between job skill 'communication' and profile skill 'resume writing': 1.00\n",
            "Similarity between job skill 'communication' and profile skill 'sales': 1.00\n",
            "Similarity between job skill 'communication' and profile skill 'sourcing': 1.00\n",
            "Similarity between job skill 'communication' and profile skill 'ventures': 1.00\n",
            "Similarity between job skill 'communication' and profile skill 'statistical data analysis': 1.00\n",
            "Similarity between job skill 'communication' and profile skill 'statistics': 1.00\n",
            "Similarity between job skill 'communication' and profile skill 'strategic sourcing': 1.00\n",
            "Similarity between job skill 'communication' and profile skill 'strategy': 1.00\n",
            "Similarity between job skill 'communication' and profile skill 'vacancies': 1.00\n",
            "Similarity between job skill 'communication' and profile skill 'venture capital': 1.00\n",
            "Similarity between job skill 'proficient' and profile skill 'analytical skills': 0.43\n",
            "Similarity between job skill 'proficient' and profile skill 'candidate management': 0.18\n",
            "Similarity between job skill 'proficient' and profile skill '': 0.28\n",
            "Similarity between job skill 'proficient' and profile skill 'communication': 0.28\n",
            "Similarity between job skill 'proficient' and profile skill 'corporate recruiting': 0.29\n",
            "Similarity between job skill 'proficient' and profile skill 'curriculum vitae cv': 0.25\n",
            "Similarity between job skill 'proficient' and profile skill 'english': 0.35\n",
            "Similarity between job skill 'proficient' and profile skill 'executive search': 0.23\n",
            "Similarity between job skill 'proficient' and profile skill 'game theory': 0.14\n",
            "Similarity between job skill 'proficient' and profile skill 'hiring': 0.42\n",
            "Similarity between job skill 'proficient' and profile skill 'human resources hr': 0.18\n",
            "Similarity between job skill 'proficient' and profile skill 'interpersonal skills': 0.35\n",
            "Similarity between job skill 'proficient' and profile skill 'interview preparation': 0.28\n",
            "Similarity between job skill 'proficient' and profile skill 'leadership': 0.28\n",
            "Similarity between job skill 'proficient' and profile skill 'linkedin': 0.21\n",
            "Similarity between job skill 'proficient' and profile skill 'microsoft excel': 0.13\n",
            "Similarity between job skill 'proficient' and profile skill 'problem solving': 0.24\n",
            "Similarity between job skill 'proficient' and profile skill 'recruiting': 0.39\n",
            "Similarity between job skill 'proficient' and profile skill '': 0.28\n",
            "Similarity between job skill 'proficient' and profile skill 'resume writing': 0.29\n",
            "Similarity between job skill 'proficient' and profile skill 'sales': 0.28\n",
            "Similarity between job skill 'proficient' and profile skill 'sourcing': 0.24\n",
            "Similarity between job skill 'proficient' and profile skill 'ventures': 0.31\n",
            "Similarity between job skill 'proficient' and profile skill 'statistical data analysis': 0.18\n",
            "Similarity between job skill 'proficient' and profile skill 'statistics': 0.22\n",
            "Similarity between job skill 'proficient' and profile skill 'strategic sourcing': 0.16\n",
            "Similarity between job skill 'proficient' and profile skill 'strategy': 0.26\n",
            "Similarity between job skill 'proficient' and profile skill 'vacancies': 0.32\n",
            "Similarity between job skill 'proficient' and profile skill 'venture capital': 0.22\n",
            "Similarity between job skill 'office' and profile skill 'analytical skills': 0.19\n",
            "Similarity between job skill 'office' and profile skill 'candidate management': 0.32\n",
            "Similarity between job skill 'office' and profile skill '': 0.30\n",
            "Similarity between job skill 'office' and profile skill 'communication': 0.37\n",
            "Similarity between job skill 'office' and profile skill 'corporate recruiting': 0.30\n",
            "Similarity between job skill 'office' and profile skill 'curriculum vitae cv': 0.12\n",
            "Similarity between job skill 'office' and profile skill 'english': 0.40\n",
            "Similarity between job skill 'office' and profile skill 'executive search': 0.34\n",
            "Similarity between job skill 'office' and profile skill 'game theory': 0.13\n",
            "Similarity between job skill 'office' and profile skill 'hiring': 0.48\n",
            "Similarity between job skill 'office' and profile skill 'human resources hr': 0.41\n",
            "Similarity between job skill 'office' and profile skill 'interpersonal skills': 0.23\n",
            "Similarity between job skill 'office' and profile skill 'interview preparation': 0.20\n",
            "Similarity between job skill 'office' and profile skill 'leadership': 0.39\n",
            "Similarity between job skill 'office' and profile skill 'linkedin': 0.22\n",
            "Similarity between job skill 'office' and profile skill 'microsoft excel': 0.34\n",
            "Similarity between job skill 'office' and profile skill 'problem solving': 0.28\n",
            "Similarity between job skill 'office' and profile skill 'recruiting': 0.29\n",
            "Similarity between job skill 'office' and profile skill '': 0.30\n",
            "Similarity between job skill 'office' and profile skill 'resume writing': 0.28\n",
            "Similarity between job skill 'office' and profile skill 'sales': 0.48\n",
            "Similarity between job skill 'office' and profile skill 'sourcing': 0.33\n",
            "Similarity between job skill 'office' and profile skill 'ventures': 0.30\n",
            "Similarity between job skill 'office' and profile skill 'statistical data analysis': 0.17\n",
            "Similarity between job skill 'office' and profile skill 'statistics': 0.27\n",
            "Similarity between job skill 'office' and profile skill 'strategic sourcing': 0.21\n",
            "Similarity between job skill 'office' and profile skill 'strategy': 0.25\n",
            "Similarity between job skill 'office' and profile skill 'vacancies': 0.32\n",
            "Similarity between job skill 'office' and profile skill 'venture capital': 0.24\n",
            "Similarity between job skill 'team management' and profile skill 'analytical skills': 1.00\n",
            "Similarity between job skill 'team management' and profile skill 'candidate management': 1.00\n",
            "Similarity between job skill 'team management' and profile skill '': 1.00\n",
            "Similarity between job skill 'team management' and profile skill 'communication': 1.00\n",
            "Similarity between job skill 'team management' and profile skill 'corporate recruiting': 1.00\n",
            "Similarity between job skill 'team management' and profile skill 'curriculum vitae cv': 1.00\n",
            "Similarity between job skill 'team management' and profile skill 'english': 1.00\n",
            "Similarity between job skill 'team management' and profile skill 'executive search': 1.00\n",
            "Similarity between job skill 'team management' and profile skill 'game theory': 1.00\n",
            "Similarity between job skill 'team management' and profile skill 'hiring': 1.00\n",
            "Similarity between job skill 'team management' and profile skill 'human resources hr': 1.00\n",
            "Similarity between job skill 'team management' and profile skill 'interpersonal skills': 1.00\n",
            "Similarity between job skill 'team management' and profile skill 'interview preparation': 1.00\n",
            "Similarity between job skill 'team management' and profile skill 'leadership': 1.00\n",
            "Similarity between job skill 'team management' and profile skill 'linkedin': 1.00\n",
            "Similarity between job skill 'team management' and profile skill 'microsoft excel': 1.00\n",
            "Similarity between job skill 'team management' and profile skill 'problem solving': 1.00\n",
            "Similarity between job skill 'team management' and profile skill 'recruiting': 1.00\n",
            "Similarity between job skill 'team management' and profile skill '': 1.00\n",
            "Similarity between job skill 'team management' and profile skill 'resume writing': 1.00\n",
            "Similarity between job skill 'team management' and profile skill 'sales': 1.00\n",
            "Similarity between job skill 'team management' and profile skill 'sourcing': 1.00\n",
            "Similarity between job skill 'team management' and profile skill 'ventures': 1.00\n",
            "Similarity between job skill 'team management' and profile skill 'statistical data analysis': 1.00\n",
            "Similarity between job skill 'team management' and profile skill 'statistics': 1.00\n",
            "Similarity between job skill 'team management' and profile skill 'strategic sourcing': 1.00\n",
            "Similarity between job skill 'team management' and profile skill 'strategy': 1.00\n",
            "Similarity between job skill 'team management' and profile skill 'vacancies': 1.00\n",
            "Similarity between job skill 'team management' and profile skill 'venture capital': 1.00\n",
            "\n",
            "Overall Similarity: 0.56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_scores = []\n",
        "\n",
        "for j, candidate_skill in enumerate(profile_skill):\n",
        "    max_score = 0\n",
        "    for i, job_skill in enumerate(job_skills_sentences):\n",
        "        similarity_score = cosine_similarities[i][j].item()\n",
        "        if similarity_score > max_score:\n",
        "            max_score = similarity_score\n",
        "    max_scores.append(max_score)\n",
        "\n",
        "overall_score = np.mean(max_scores)\n",
        "\n",
        "print(f\"Overall similarity score for the candidate: {overall_score:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WI-K1WbgkGdK",
        "outputId": "12172937-aa83-4ab3-d670-226d9c9db201"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall similarity score for the candidate: 0.26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "nY1-Fn97WgoN"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Ensure the input is a string (not tokenized yet)\n",
        "    if isinstance(text, list):  # If it's already tokenized, return as-is\n",
        "        return text\n",
        "    # Lowercase and remove non-alphanumeric characters\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text.lower())\n",
        "    # Tokenize\n",
        "    words = text.split()\n",
        "    # Remove stopwords\n",
        "    words = [word for word in words if word not in stopwords.words('english')]\n",
        "    # Lemmatize\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    return words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "J4TR1IklWqp8"
      },
      "outputs": [],
      "source": [
        "# Apply to CV and JD\n",
        "input_CV = preprocess_text(profile_combined)\n",
        "input_JD = preprocess_text(jobs_combined)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# def get_tfidf_weighted_vector(model, document, tfidf_weights):\n",
        "#     words = document.split()\n",
        "#     word_vectors = [model.wv[word] * tfidf_weights.get(word, 0) for word in words if word in model.wv]\n",
        "#     if not word_vectors:\n",
        "#         return np.zeros(model.vector_size)\n",
        "#     return np.mean(word_vectors, axis=0)\n",
        "\n",
        "# # Example TF-IDF setup\n",
        "# tfidf = TfidfVectorizer()\n",
        "# tfidf.fit([input_CV, input_JD])  # Fit on both the CV and Job Description\n",
        "# tfidf_weights = dict(zip(tfidf.get_feature_names_out(), tfidf.idf_))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "RTEot4czobsp",
        "outputId": "39866939-951a-4599-fa49-87edc5bb6621"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'lower'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-175ed10d5831>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Example TF-IDF setup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_CV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_JD\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Fit on both the CV and Job Description\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mtfidf_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midf_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2072\u001b[0m             \u001b[0msublinear_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msublinear_tf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2073\u001b[0m         )\n\u001b[0;32m-> 2074\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2075\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2076\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1374\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1376\u001b[0;31m         \u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1261\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1263\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1264\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_preprocess\u001b[0;34m(doc, accent_function, lower)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \"\"\"\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maccent_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccent_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "model = SentenceTransformer('cv_job_maching.model')  # Pretrained model for semantic similarity\n",
        "\n",
        "def encode_document(model, document):\n",
        "    \"\"\"\n",
        "    Encodes a document (string or list of sentences) into a single vector\n",
        "    by averaging the sentence embeddings.\n",
        "    \"\"\"\n",
        "    embeddings = model.encode(document)\n",
        "    if len(embeddings.shape) == 2:  # If multiple sentence embeddings\n",
        "        return np.mean(embeddings, axis=0)  # Average them\n",
        "    return embeddings  # Return as is if already a single vector\n",
        "\n",
        "# Encode the LinkedIn profile and job description\n",
        "v1 = encode_document(model, input_CV)\n",
        "v2 = encode_document(model, input_JD)\n",
        "\n",
        "# Compute cosine similarity\n",
        "similarity = cosine_similarity([v1], [v2])[0][0] * 100\n",
        "print(f\"Similarity: {round(similarity, 2)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 897
        },
        "id": "NMBNi-LNoqiX",
        "outputId": "7124a970-a2f9-45de-b90c-bfaf23e9f770"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name cv_job_maching.model. Creating a new one with mean pooling.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "cv_job_maching.model is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/cv_job_maching.model/resolve/main/adapter_config.json",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m    861\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m    966\u001b[0m         \u001b[0;31m# Otherwise, raise appropriate error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m         \u001b[0m_raise_on_head_call_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_call_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    968\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1481\u001b[0m         \u001b[0;31m# Repo not found or gated => let's raise the actual error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1482\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mhead_call_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1483\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1373\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1375\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1293\u001b[0m     \u001b[0;31m# Retrieve metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1294\u001b[0;31m     r = _request_wrapper(\n\u001b[0m\u001b[1;32m   1295\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfollow_relative_redirects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m         response = _request_wrapper(\n\u001b[0m\u001b[1;32m    279\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    453\u001b[0m             )\n\u001b[0;32m--> 454\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRepositoryNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-6798a3f9-2d84ca474fe3eea63d0014fc;03155697-6286-4a40-b00f-4b87d621bad7)\n\nRepository Not Found for url: https://huggingface.co/cv_job_maching.model/resolve/main/adapter_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-03783eb594a5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cv_job_maching.model'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Pretrained model for semantic similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mencode_document\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[0m\n\u001b[1;32m    318\u001b[0m                 )\n\u001b[1;32m    319\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                 modules = self._load_auto_model(\n\u001b[0m\u001b[1;32m    321\u001b[0m                     \u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                     \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36m_load_auto_model\u001b[0;34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs)\u001b[0m\n\u001b[1;32m   1526\u001b[0m         \u001b[0mconfig_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshared_kwargs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mconfig_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mshared_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mconfig_kwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1528\u001b[0;31m         transformer_model = Transformer(\n\u001b[0m\u001b[1;32m   1529\u001b[0m             \u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_folder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/models/Transformer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_name_or_path, max_seq_length, model_args, tokenizer_args, config_args, cache_dir, do_lower_case, tokenizer_name_or_path, backend)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mconfig_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/models/Transformer.py\u001b[0m in \u001b[0;36m_load_config\u001b[0;34m(self, model_name_or_path, cache_dir, backend, config_args)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;34m\"\"\"Loads the configuration of a model\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         if (\n\u001b[0;32m--> 105\u001b[0;31m             find_adapter_config_file(\n\u001b[0m\u001b[1;32m    106\u001b[0m                 \u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"token\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/peft_utils.py\u001b[0m in \u001b[0;36mfind_adapter_config_file\u001b[0;34m(model_id, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, _commit_hash)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0madapter_cached_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mADAPTER_CONFIG_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         adapter_cached_filename = cached_file(\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mADAPTER_CONFIG_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    424\u001b[0m         ) from e\n\u001b[1;32m    425\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mRepositoryNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         raise EnvironmentError(\n\u001b[0m\u001b[1;32m    427\u001b[0m             \u001b[0;34mf\"{path_or_repo_id} is not a local folder and is not a valid model identifier \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0;34m\"listed on 'https://huggingface.co/models'\\nIf this is a private repository, make sure to pass a token \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: cv_job_maching.model is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import AutoModel, AutoTokenizer\n",
        "# import torch\n",
        "\n",
        "# # Load SciBERT model and tokenizer\n",
        "# tokenizer = AutoTokenizer.from_pretrained('cv_job_maching.model')\n",
        "# model = AutoModel.from_pretrained('cv_job_maching.model')\n",
        "\n",
        "# def preprocess_input(text):\n",
        "#     # Ensure input is a string\n",
        "#     if isinstance(text, list):\n",
        "#         text = ' '.join(text)  # Join list into string\n",
        "#     # Convert to lowercase and apply additional preprocessing if needed\n",
        "#     return text.lower()\n",
        "\n",
        "# def encode_text(text):\n",
        "#     # Preprocess the input text\n",
        "#     text = preprocess_input(text)\n",
        "#     # Tokenize and encode\n",
        "#     inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
        "#     with torch.no_grad():\n",
        "#         outputs = model(**inputs)\n",
        "#     # Return the mean of the last hidden state as the text embedding\n",
        "#     return outputs.last_hidden_state.mean(dim=1).squeeze()\n",
        "\n",
        "\n",
        "\n",
        "# # Process and encode inputs\n",
        "# v1 = encode_text(input_CV)\n",
        "# v2 = encode_text(input_JD)\n",
        "\n",
        "# # Compute cosine similarity\n",
        "# similarity = torch.nn.functional.cosine_similarity(v1, v2, dim=0)\n",
        "# print(f\"Similarity: {similarity.item() * 100:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 879
        },
        "id": "eYf4RnwzsINI",
        "outputId": "a1820a76-400b-4c27-b316-073fb740cb5d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "cv_job_maching.model is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/cv_job_maching.model/resolve/main/tokenizer_config.json",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m    861\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m    966\u001b[0m         \u001b[0;31m# Otherwise, raise appropriate error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m         \u001b[0m_raise_on_head_call_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_call_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    968\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1481\u001b[0m         \u001b[0;31m# Repo not found or gated => let's raise the actual error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1482\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mhead_call_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1483\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1373\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1375\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1293\u001b[0m     \u001b[0;31m# Retrieve metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1294\u001b[0;31m     r = _request_wrapper(\n\u001b[0m\u001b[1;32m   1295\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfollow_relative_redirects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m         response = _request_wrapper(\n\u001b[0m\u001b[1;32m    279\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    453\u001b[0m             )\n\u001b[0;32m--> 454\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRepositoryNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-6797e3a3-5ce5b2973e9cca756d02f6ae;e0a45740-453b-4072-a110-058a0418d6c3)\n\nRepository Not Found for url: https://huggingface.co/cv_job_maching.model/resolve/main/tokenizer_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-dd6160e686c4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load SciBERT model and tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cv_job_maching.model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cv_job_maching.model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    856\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;31m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 858\u001b[0;31m         \u001b[0mtokenizer_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tokenizer_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    859\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"_commit_hash\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[0mcommit_hash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m     resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    691\u001b[0m         \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m         \u001b[0mTOKENIZER_CONFIG_FILE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    424\u001b[0m         ) from e\n\u001b[1;32m    425\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mRepositoryNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         raise EnvironmentError(\n\u001b[0m\u001b[1;32m    427\u001b[0m             \u001b[0;34mf\"{path_or_repo_id} is not a local folder and is not a valid model identifier \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0;34m\"listed on 'https://huggingface.co/models'\\nIf this is a private repository, make sure to pass a token \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: cv_job_maching.model is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7dKopfNPTym"
      },
      "source": [
        "#### 7. Matching\n",
        "Using the trained model, we infer the document vectors for the resume and job description. Then, we calculate the cosine similarity between the two vectors to determine the match between the resume and the job description."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlbOvGLfWqsd",
        "outputId": "b9d46ff9-6caa-43a5-d76c-6fff226055e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Word2Vec' object has no attribute 'infer_vector'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-708763872e74>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Model evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDoc2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cv_job_maching.model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_CV\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_JD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msimilarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Word2Vec' object has no attribute 'infer_vector'"
          ]
        }
      ],
      "source": [
        "# # Model evaluation\n",
        "# model = Doc2Vec.load('cv_job_maching.model')\n",
        "# v1 = model.infer_vector(input_CV.split())\n",
        "# v2 = model.infer_vector(input_JD.split())\n",
        "# similarity = 100*(np.dot(np.array(v1), np.array(v2))) / (norm(np.array(v1)) * norm(np.array(v2)))\n",
        "# print(round(similarity, 2))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from numpy.linalg import norm\n",
        "# import numpy as np\n",
        "\n",
        "# # Load the trained Word2Vec model\n",
        "# model = Word2Vec.load('cv_job_maching.model')\n",
        "\n",
        "# def get_document_vector(model, document):\n",
        "#     \"\"\"\n",
        "#     Compute the document vector by averaging word vectors.\n",
        "#     Words not in the model's vocabulary are ignored.\n",
        "#     \"\"\"\n",
        "#     words = document.split()\n",
        "#     word_vectors = [model.wv[word] for word in words if word in model.wv]\n",
        "#     if not word_vectors:  # Handle empty or unknown words\n",
        "#         return np.zeros(model.vector_size)\n",
        "#     return np.mean(word_vectors, axis=0)\n",
        "\n",
        "# # Compute document vectors for the input CV and Job Description\n",
        "# v1 = get_document_vector(model, input_CV)\n",
        "# v2 = get_document_vector(model, input_JD)\n",
        "\n",
        "# # Compute cosine similarity\n",
        "# similarity = 100 * np.dot(v1, v2) / (norm(v1) * norm(v2))\n",
        "# print(round(similarity, 2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "uSHVg203l68o",
        "outputId": "ad452dc1-10f6-4f7c-fa4f-4ec5f067dd29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'split'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-acc96bf80e67>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Compute document vectors for the input CV and Job Description\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_document_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_CV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_document_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_JD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-acc96bf80e67>\u001b[0m in \u001b[0;36mget_document_vector\u001b[0;34m(model, document)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mWords\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mvocabulary\u001b[0m \u001b[0mare\u001b[0m \u001b[0mignored\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \"\"\"\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mword_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mword_vectors\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Handle empty or unknown words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "utXOI-fPU7fS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFDN3UbLPTyn"
      },
      "source": [
        "#### 8. Visualization and Notification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "cCjPrtxYPTyn",
        "outputId": "860a33c5-2b5a-4270-e9fb-bed18f6402b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"56886861-99e4-4fce-9f8b-bdedaa9f2d3f\" class=\"plotly-graph-div\" style=\"height:400px; width:600px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"56886861-99e4-4fce-9f8b-bdedaa9f2d3f\")) {                    Plotly.newPlot(                        \"56886861-99e4-4fce-9f8b-bdedaa9f2d3f\",                        [{\"domain\":{\"x\":[0,1],\"y\":[0,1]},\"gauge\":{\"axis\":{\"range\":[0,100]},\"steps\":[{\"color\":\"#FFB6C1\",\"range\":[0,50]},{\"color\":\"#FFFFE0\",\"range\":[50,70]},{\"color\":\"#90EE90\",\"range\":[70,100]}],\"threshold\":{\"line\":{\"color\":\"red\",\"width\":4},\"thickness\":0.75,\"value\":100}},\"mode\":\"gauge+number\",\"title\":{\"text\":\"Matching percentage (%)\"},\"value\":95.50549983978271,\"type\":\"indicator\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"width\":600,\"height\":400},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('56886861-99e4-4fce-9f8b-bdedaa9f2d3f');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Excellent! You can submit your CV.\n"
          ]
        }
      ],
      "source": [
        "# Visualization\n",
        "fig = go.Figure(go.Indicator(\n",
        "    domain = {'x': [0, 1], 'y': [0, 1]},\n",
        "    value = similarity,\n",
        "    mode = \"gauge+number\",\n",
        "    title = {'text': \"Matching percentage (%)\"},\n",
        "    #delta = {'reference': 100},\n",
        "    gauge = {\n",
        "        'axis': {'range': [0, 100]},\n",
        "        'steps' : [\n",
        "            {'range': [0, 50], 'color': \"#FFB6C1\"},\n",
        "            {'range': [50, 70], 'color': \"#FFFFE0\"},\n",
        "            {'range': [70, 100], 'color': \"#90EE90\"}\n",
        "        ],\n",
        "             'threshold' : {'line': {'color': \"red\", 'width': 4}, 'thickness': 0.75, 'value': 100}}))\n",
        "\n",
        "fig.update_layout(width=600, height=400)  # Adjust the width and height as desired\n",
        "fig.show()\n",
        "\n",
        "# Print notification\n",
        "if similarity < 50:\n",
        "    print(colored(\"Low chance, need to modify your CV!\", \"red\", attrs=[\"bold\"]))\n",
        "elif similarity >= 50 and similarity < 80:\n",
        "    print(colored(\"Good chance but you can improve further!\", \"yellow\", attrs=[\"bold\"]))\n",
        "else:\n",
        "    print(colored(\"Excellent! You can submit your CV.\", \"green\", attrs=[\"bold\"]))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
