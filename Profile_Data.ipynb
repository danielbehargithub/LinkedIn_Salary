{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyObqZ5CzDEUIBAQT3TYqhz/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielbehargithub/LinkedIn_Salary/blob/main/Profile_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install playwright\n",
        "!playwright install\n"
      ],
      "metadata": {
        "id": "EEQyMLpja1MB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enter url, email and password\n",
        "user_url = \"https://www.linkedin.com/in/profile_url\"\n",
        "email = \"email@gmail.com\"\n",
        "password = \"pass\"\n"
      ],
      "metadata": {
        "id": "6CBji7t4eWH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "from playwright.async_api import async_playwright\n",
        "import asyncio\n",
        "\n",
        "async def scrape_user_profile(user_url, email, password):\n",
        "    async with async_playwright() as p:\n",
        "        browser = await p.chromium.launch(headless=True)  # Running in headless mode\n",
        "        page = await browser.new_page()\n",
        "\n",
        "        print(\"Navigating to LinkedIn login page...\")\n",
        "        # Navigate to LinkedIn login page\n",
        "        await page.goto(\"https://www.linkedin.com/login\")\n",
        "        await asyncio.sleep(3)\n",
        "\n",
        "        # Perform login\n",
        "        print(\"Logging into LinkedIn...\")\n",
        "        await page.fill('input[id=\"username\"]', email)\n",
        "        await page.fill('input[id=\"password\"]', password)\n",
        "        await page.click('button[type=\"submit\"]')\n",
        "        await asyncio.sleep(5)\n",
        "\n",
        "        # Check for two-step verification\n",
        "        if await page.is_visible('input[name=\"pin\"]'):\n",
        "            print(\"Two-step verification detected. Please enter the code sent to your email.\")\n",
        "            verification_code = input(\"Enter the verification code: \")\n",
        "            await page.fill('input[name=\"pin\"]', verification_code)\n",
        "            await page.click('button[type=\"submit\"]')\n",
        "            await asyncio.sleep(3)\n",
        "\n",
        "        # Relevant sections to scrape for the user\n",
        "        profile_sections = [\n",
        "            \"/details/skills/\",\n",
        "            \"/details/interests/\",\n",
        "            \"/details/recommendations/\",\n",
        "            \"/details/certifications/\"\n",
        "        ]\n",
        "\n",
        "        scraped_data = {}  # Dictionary to store HTML content of each section\n",
        "\n",
        "        # Loop through the list of profile URLs\n",
        "        for section in profile_sections:\n",
        "            url = f\"{user_url}{section}\"\n",
        "            try:\n",
        "                print(f\"Navigating to section: {url}\")\n",
        "                # Navigate to the section URL\n",
        "                await page.goto(url)\n",
        "                await asyncio.sleep(5)\n",
        "\n",
        "                # Save the HTML content of the section page\n",
        "                html = await page.content()\n",
        "                section_name = section.strip(\"/\").split(\"/\")[-1]  # Keep only the part after the last slash\n",
        "                scraped_data[section_name] = html  # Store HTML content in dictionary\n",
        "                print(f\"HTML content for {section_name} saved in memory.\")\n",
        "\n",
        "                # For debug\n",
        "                filename = f\"{section_name}.html\"  # Ensure .html extension\n",
        "                with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "                    f.write(html)\n",
        "                print(f\"Saved HTML for {url} as {section_name}\")\n",
        "\n",
        "                # Optional: Take a screenshot of the section page\n",
        "                screenshot_filename = f\"{section_name}.png\"\n",
        "                await page.screenshot(path=screenshot_filename, full_page=True)\n",
        "                print(f\"Saved screenshot for {url} as {screenshot_filename}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to scrape {url}: {e}\")\n",
        "\n",
        "        # Close the browser\n",
        "        print(\"Closing the browser...\")\n",
        "        await browser.close()\n",
        "\n",
        "        return scraped_data  # Return all scraped HTML as a dictionary\n",
        "\n"
      ],
      "metadata": {
        "id": "DNftkb7QUFCh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scraped_html_data = await scrape_user_profile(user_url, email, password)\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Example HTML from the dictionary\n",
        "html_content = scraped_html_data.get(\"skills\")\n",
        "\n",
        "# Parse HTML\n",
        "soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "\n"
      ],
      "metadata": {
        "id": "glxbVQpiZTxn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find all <a> tags with data-field=\"skill_page_skill_topic\"\n",
        "skill_links = soup.find_all(\"a\", {\"data-field\": \"skill_page_skill_topic\"})\n",
        "\n",
        "# Extract skills\n",
        "skills = []\n",
        "for link in skill_links:\n",
        "    # Find the <span> with aria-hidden=\"true\" inside the link\n",
        "    skill_span = link.find(\"span\", {\"aria-hidden\": \"true\"})\n",
        "    if skill_span:\n",
        "        skill_text = skill_span.get_text(strip=True)\n",
        "        skills.append(skill_text)\n",
        "\n",
        "print(\"Extracted skills:\", skills)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLPVQEfBZ-gW",
        "outputId": "16cebd6e-0200-43ab-ed0c-82b46375c4e2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted skills: ['Java', 'Apache Spark', 'Django', 'Data Structures', 'PyTorch', 'Deep Learning', 'Statistical Data Analysis', 'Python (Programming Language)', 'SQL', 'Data Analysis', 'HTML', 'Apache Spark', 'Data Structures', 'Deep Learning', 'Statistical Data Analysis', 'Data Analysis', 'Java', 'Django', 'PyTorch', 'Python (Programming Language)', 'SQL', 'HTML']\n"
          ]
        }
      ]
    }
  ]
}