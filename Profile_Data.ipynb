{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNRl/WGBEUQ+0+A+l5EkIKB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielbehargithub/LinkedIn_Salary/blob/main/Profile_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install playwright\n",
        "!playwright install\n"
      ],
      "metadata": {
        "id": "EEQyMLpja1MB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enter url, email and password\n",
        "user_url = \"https://www.linkedin.com/in/daniel-behar-168647280/\"\n",
        "email = \"daniel10behar@gmail.com\"\n",
        "password = \"PASS\"\n"
      ],
      "metadata": {
        "id": "6CBji7t4eWH9"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "from playwright.async_api import async_playwright\n",
        "import asyncio\n",
        "\n",
        "async def scrape_user_profile(user_url, email, password):\n",
        "    async with async_playwright() as p:\n",
        "        browser = await p.chromium.launch(headless=True)  # Running in headless mode\n",
        "        page = await browser.new_page()\n",
        "\n",
        "        print(\"Navigating to LinkedIn login page...\")\n",
        "        # Navigate to LinkedIn login page\n",
        "        await page.goto(\"https://www.linkedin.com/login\")\n",
        "        await asyncio.sleep(3)\n",
        "\n",
        "        # Perform login\n",
        "        print(\"Logging into LinkedIn...\")\n",
        "        await page.fill('input[id=\"username\"]', email)\n",
        "        await page.fill('input[id=\"password\"]', password)\n",
        "        await page.click('button[type=\"submit\"]')\n",
        "        await asyncio.sleep(5)\n",
        "\n",
        "        # Check for two-step verification\n",
        "        if await page.is_visible('input[name=\"pin\"]'):\n",
        "            print(\"Two-step verification detected. Please enter the code sent to your email.\")\n",
        "            verification_code = input(\"Enter the verification code: \")\n",
        "            await page.fill('input[name=\"pin\"]', verification_code)\n",
        "            await page.click('button[type=\"submit\"]')\n",
        "            await asyncio.sleep(3)\n",
        "\n",
        "        # Relevant sections to scrape for the user\n",
        "        profile_sections = [\n",
        "            \"/details/skills/\",\n",
        "            \"/details/interests/\",\n",
        "            \"/details/recommendations/\",\n",
        "            \"/details/certifications/\"\n",
        "        ]\n",
        "\n",
        "        scraped_data = {}  # Dictionary to store HTML content of each section\n",
        "\n",
        "        # Loop through the list of profile URLs\n",
        "        for section in profile_sections:\n",
        "            url = f\"{user_url}{section}\"\n",
        "            try:\n",
        "                print(f\"Navigating to section: {url}\")\n",
        "                # Navigate to the section URL\n",
        "                await page.goto(url)\n",
        "                await asyncio.sleep(5)\n",
        "\n",
        "                # Save the HTML content of the section page\n",
        "                html = await page.content()\n",
        "                section_name = section.strip(\"/\").split(\"/\")[-1]  # Keep only the part after the last slash\n",
        "                scraped_data[section_name] = html  # Store HTML content in dictionary\n",
        "                print(f\"HTML content for {section_name} saved in memory.\")\n",
        "\n",
        "                # For debug\n",
        "                filename = f\"{section_name}.html\"  # Ensure .html extension\n",
        "                with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "                    f.write(html)\n",
        "                print(f\"Saved HTML for {url} as {section_name}\")\n",
        "\n",
        "                # Optional: Take a screenshot of the section page\n",
        "                screenshot_filename = f\"{section_name}.png\"\n",
        "                await page.screenshot(path=screenshot_filename, full_page=True)\n",
        "                print(f\"Saved screenshot for {url} as {screenshot_filename}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to scrape {url}: {e}\")\n",
        "\n",
        "        # Close the browser\n",
        "        print(\"Closing the browser...\")\n",
        "        await browser.close()\n",
        "\n",
        "        return scraped_data  # Return all scraped HTML as a dictionary\n",
        "\n"
      ],
      "metadata": {
        "id": "DNftkb7QUFCh"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scraped_html_data = await scrape_user_profile(user_url, email, password)\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Example HTML from the dictionary\n",
        "html_content = scraped_html_data.get(\"skills\")\n",
        "\n",
        "# Parse HTML\n",
        "soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "\n"
      ],
      "metadata": {
        "id": "glxbVQpiZTxn",
        "outputId": "9166fcdb-b427-457d-f94d-a4035987dada",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Navigating to LinkedIn login page...\n",
            "Logging into LinkedIn...\n",
            "Navigating to section: https://www.linkedin.com/in/daniel-behar-168647280//details/skills/\n",
            "HTML content for skills saved in memory.\n",
            "Saved HTML for https://www.linkedin.com/in/daniel-behar-168647280//details/skills/ as skills\n",
            "Saved screenshot for https://www.linkedin.com/in/daniel-behar-168647280//details/skills/ as skills.png\n",
            "Navigating to section: https://www.linkedin.com/in/daniel-behar-168647280//details/interests/\n",
            "HTML content for interests saved in memory.\n",
            "Saved HTML for https://www.linkedin.com/in/daniel-behar-168647280//details/interests/ as interests\n",
            "Saved screenshot for https://www.linkedin.com/in/daniel-behar-168647280//details/interests/ as interests.png\n",
            "Navigating to section: https://www.linkedin.com/in/daniel-behar-168647280//details/recommendations/\n",
            "HTML content for recommendations saved in memory.\n",
            "Saved HTML for https://www.linkedin.com/in/daniel-behar-168647280//details/recommendations/ as recommendations\n",
            "Saved screenshot for https://www.linkedin.com/in/daniel-behar-168647280//details/recommendations/ as recommendations.png\n",
            "Navigating to section: https://www.linkedin.com/in/daniel-behar-168647280//details/certifications/\n",
            "HTML content for certifications saved in memory.\n",
            "Saved HTML for https://www.linkedin.com/in/daniel-behar-168647280//details/certifications/ as certifications\n",
            "Saved screenshot for https://www.linkedin.com/in/daniel-behar-168647280//details/certifications/ as certifications.png\n",
            "Closing the browser...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find all <a> tags with data-field=\"skill_page_skill_topic\"\n",
        "skill_links = soup.find_all(\"a\", {\"data-field\": \"skill_page_skill_topic\"})\n",
        "\n",
        "# Extract skills\n",
        "skills = []\n",
        "for link in skill_links:\n",
        "    # Find the <span> with aria-hidden=\"true\" inside the link\n",
        "    skill_span = link.find(\"span\", {\"aria-hidden\": \"true\"})\n",
        "    if skill_span:\n",
        "        skill_text = skill_span.get_text(strip=True)\n",
        "        skills.append(skill_text)\n",
        "\n",
        "unique_skills = sorted(set(skills))\n",
        "print(\"Unique skills:\", unique_skills)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLPVQEfBZ-gW",
        "outputId": "423e1131-75c3-4855-c57e-0108dbd8a84b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique skills: ['Apache Spark', 'Data Analysis', 'Data Structures', 'Deep Learning', 'Django', 'HTML', 'Java', 'PyTorch', 'Python (Programming Language)', 'SQL', 'Statistical Data Analysis']\n"
          ]
        }
      ]
    }
  ]
}