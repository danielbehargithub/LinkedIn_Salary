{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOyUqs47PqgWRl52GbVBiYD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielbehargithub/LinkedIn_Salary/blob/main/Profile_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then click on Runtime -> Run all.\n",
        "\n",
        "Please enter your LinkedIn credentials and job url.\n",
        "\n",
        "You will receive your LinkedIn and job data.\n",
        "\n",
        "For full guide go to README file."
      ],
      "metadata": {
        "id": "u_Ks64n0Zyvd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example:\n",
        "\n",
        "user url: https://www.linkedin.com/in/profile_id/\n",
        "\n",
        "email: my_email@gmail.com\n",
        "\n",
        "password: my_password\n",
        "\n",
        "job url: https://www.linkedin.com/jobs/view/job_number/"
      ],
      "metadata": {
        "id": "JBKQ4IYswRGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "\n",
        "\n",
        "# Enter url, email and password\n",
        "user_url = input(\"Please enter full LinkedIn profile url: \").strip()\n",
        "email = input(\"Please enter LinkedIn email: \").strip()\n",
        "password = getpass(\"Please enter your LinkedIn password: \")\n",
        "print(\"Password received securely.\")\n",
        "job_url = input(\"Please enter the LinkedIn job URL: \").strip()\n"
      ],
      "metadata": {
        "id": "6CBji7t4eWH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install playwright\n",
        "!playwright install\n"
      ],
      "metadata": {
        "id": "EEQyMLpja1MB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "from playwright.async_api import async_playwright\n",
        "import asyncio\n",
        "\n",
        "async def scrape_user_profile(user_url, email, password):\n",
        "    async with async_playwright() as p:\n",
        "        browser = await p.chromium.launch(headless=True)  # Running in headless mode\n",
        "        page = await browser.new_page()\n",
        "\n",
        "        print(\"Navigating to LinkedIn login page...\")\n",
        "        # Navigate to LinkedIn login page\n",
        "        await page.goto(\"https://www.linkedin.com/login\")\n",
        "        await asyncio.sleep(3)\n",
        "\n",
        "        # Perform login\n",
        "        print(\"Logging into LinkedIn...\")\n",
        "        print(\"*\" * 50)\n",
        "\n",
        "        await page.fill('input[id=\"username\"]', email)\n",
        "        await page.fill('input[id=\"password\"]', password)\n",
        "        await page.click('button[type=\"submit\"]')\n",
        "        await asyncio.sleep(5)\n",
        "\n",
        "        # Check for two-step verification\n",
        "        if await page.is_visible('input[name=\"pin\"]'):\n",
        "            print(\"Two-step verification detected. Please enter the code sent to your email.\")\n",
        "            verification_code = input(\"Enter the verification code: \")\n",
        "            await page.fill('input[name=\"pin\"]', verification_code)\n",
        "            await page.click('button[type=\"submit\"]')\n",
        "            await asyncio.sleep(3)\n",
        "\n",
        "        # Relevant sections to scrape for the user\n",
        "        profile_sections = [\n",
        "            \"details/education/\",\n",
        "            \"details/skills/\",\n",
        "            \"details/experience/\"\n",
        "        ]\n",
        "\n",
        "        scraped_data = {}  # Dictionary to store HTML content of each section\n",
        "\n",
        "        MAX_RETRIES = 3\n",
        "\n",
        "        # Loop through the list of profile URLs\n",
        "        for section in profile_sections:\n",
        "            url = f\"{user_url}{section}\"\n",
        "            retries = 0\n",
        "\n",
        "            while retries < MAX_RETRIES:\n",
        "                try:\n",
        "                    print(f\"Navigating to section: {url} (Attempt {retries + 1})\")\n",
        "\n",
        "                    # Navigate to the section URL\n",
        "                    await page.goto(url)\n",
        "                    await asyncio.sleep(3)\n",
        "\n",
        "                    # Get page title to verify if the correct page is loaded\n",
        "                    page_title = await page.title()\n",
        "                    section_name = section.strip(\"/\").split(\"/\")[-1]  # Extract section name\n",
        "                    # print(f\"Page title: {page_title}\")\n",
        "                    print(f\"Section name: {section_name}\")\n",
        "\n",
        "                    # Verify if the page is correct\n",
        "                    if section_name.capitalize() not in page_title:\n",
        "                        print(f\"Page {url} did not load correctly. Retrying...\")\n",
        "                        retries += 1\n",
        "                        await asyncio.sleep(5)  # Wait a bit before retrying\n",
        "                        continue  # Retry the same section\n",
        "\n",
        "                    # Save the HTML content of the section page\n",
        "                    html = await page.content()\n",
        "                    scraped_data[section_name] = html  # Store HTML content in dictionary\n",
        "                    print(f\"HTML content for {section_name} saved in memory.\")\n",
        "                    print(\"*\" * 50)\n",
        "\n",
        "                    # Save the HTML file for debugging\n",
        "                    # filename = f\"{section_name}.html\"\n",
        "                    # with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "                    #     f.write(html)\n",
        "                    # print(f\"Saved HTML for {url} as {section_name}\")\n",
        "\n",
        "                    # Take a screenshot of the section page\n",
        "                    # screenshot_filename = f\"{section_name}.png\"\n",
        "                    # await page.screenshot(path=screenshot_filename, full_page=True)\n",
        "                    # print(f\"Saved screenshot for {url} as {screenshot_filename}\")\n",
        "\n",
        "                    break  # If successful, exit the retry loop\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Failed to scrape {url}: {e}\")\n",
        "                    retries += 1\n",
        "                    await asyncio.sleep(5)  # Wait a bit before retrying\n",
        "\n",
        "            if retries == MAX_RETRIES:\n",
        "                print(f\"Failed to scrape {url} after {MAX_RETRIES} attempts. Skipping...\")\n",
        "\n",
        "        # Close the browser\n",
        "        print(\"Closing the browser...\")\n",
        "        await browser.close()\n",
        "\n",
        "        return scraped_data  # Return all scraped HTML as a dictionary\n",
        "\n"
      ],
      "metadata": {
        "id": "DNftkb7QUFCh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scraped_html_data = await scrape_user_profile(user_url, email, password)\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Example HTML from the dictionary\n",
        "html_skills = scraped_html_data.get(\"skills\")\n",
        "html_education = scraped_html_data.get(\"education\")\n",
        "html_experience = scraped_html_data.get(\"experience\")\n",
        "\n",
        "# Parse HTML\n",
        "soup_skill = BeautifulSoup(html_skills, \"html.parser\")\n",
        "soup_education = BeautifulSoup(html_education, \"html.parser\")\n",
        "soup_experience = BeautifulSoup(html_experience, \"html.parser\")\n"
      ],
      "metadata": {
        "id": "glxbVQpiZTxn",
        "outputId": "4d4d97ba-aee2-4b77-e396-4a1333d791d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Navigating to LinkedIn login page...\n",
            "Logging into LinkedIn...\n",
            "**************************************************\n",
            "Navigating to section: https://www.linkedin.com/in/daniel-behar-168647280/details/education/ (Attempt 1)\n",
            "Section name: education\n",
            "HTML content for education saved in memory.\n",
            "**************************************************\n",
            "Navigating to section: https://www.linkedin.com/in/daniel-behar-168647280/details/skills/ (Attempt 1)\n",
            "Section name: skills\n",
            "HTML content for skills saved in memory.\n",
            "**************************************************\n",
            "Navigating to section: https://www.linkedin.com/in/daniel-behar-168647280/details/experience/ (Attempt 1)\n",
            "Section name: experience\n",
            "HTML content for experience saved in memory.\n",
            "**************************************************\n",
            "Closing the browser...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from re import sub\n",
        "\n",
        "# Initialize the output variable to collect results\n",
        "output = \"Experience Section:\\n\\n\"\n",
        "\n",
        "# Locate the experience section in the HTML using its aria-label\n",
        "experience_section = soup_experience.find(\"main\", {\"aria-label\": \"Experience\"})\n",
        "\n",
        "# Find all job records within the experience section\n",
        "records = experience_section.find_all(\"li\", class_=\"pvs-list__paged-list-item artdeco-list__item pvs-list__item--line-separated pvs-list__item--one-column\")\n",
        "\n",
        "# Process each job record\n",
        "for record in records:\n",
        "    # Check if the record contains sub-records (multiple roles in one company)\n",
        "    sub_records = record.find_all(\"li\", class_=\"pvs-list__paged-list-item pvs-list__item--one-column\")\n",
        "\n",
        "    # Handle a single position in one company\n",
        "    if len(sub_records) == 0:\n",
        "        # Extract job title\n",
        "        job_name_container = record.find(\"div\", class_=\"display-flex align-items-center mr1 hoverable-link-text t-bold\") or \\\n",
        "                             record.find(\"div\", class_=\"display-flex align-items-center mr1 t-bold\")\n",
        "        job_name = (\n",
        "            job_name_container.find(\"span\", {\"aria-hidden\": \"true\"}).get_text(strip=True)\n",
        "            if job_name_container else \"N/A\"\n",
        "        )\n",
        "\n",
        "        # Extract company name and job type\n",
        "        job_place_and_type_container = record.find(\"span\", class_=\"t-14 t-normal\")\n",
        "        if job_place_and_type_container:\n",
        "            place_and_type = job_place_and_type_container.find(\"span\", {\"aria-hidden\": \"true\"}).get_text(strip=True)\n",
        "            if \"·\" in place_and_type:\n",
        "                company_name, job_type = [part.strip() for part in place_and_type.split(\"·\", 1)]\n",
        "            else:\n",
        "                company_name, job_type = \"N/A\", place_and_type\n",
        "        else:\n",
        "            company_name, job_type = \"N/A\", \"N/A\"\n",
        "\n",
        "        # Extract job duration and location\n",
        "        job_info_container = record.find_all(\"span\", class_=\"t-14 t-normal t-black--light\")\n",
        "        info = [span.find(\"span\", {\"aria-hidden\": \"true\"}).get_text(strip=True) for span in job_info_container if span.find(\"span\", {\"aria-hidden\": \"true\"})]\n",
        "        job_duration = info[0] if len(info) > 0 else \"N/A\"\n",
        "        job_location = info[1] if len(info) > 1 else \"N/A\"\n",
        "\n",
        "        # Extract additional content and skills\n",
        "        additional_content_container = record.find_all(\"div\", class_=\"display-flex align-items-center t-14 t-normal t-black\")\n",
        "        additional_content_and_skills = [span.find(\"span\", {\"aria-hidden\": \"true\"}).get_text(strip=True) for span in additional_content_container if span.find(\"span\", {\"aria-hidden\": \"true\"})]\n",
        "        additional_content = additional_content_and_skills[0] if len(additional_content_and_skills) > 0 else \"N/A\"\n",
        "\n",
        "        skills = additional_content_and_skills[1] if len(additional_content_and_skills) > 1 else \"N/A\"\n",
        "        skills = skills.replace(\"Skills:\", \"\").strip()\n",
        "\n",
        "        # Append results to the output in a clean format\n",
        "        output += (\n",
        "            f\"Job Title     : {job_name}\\n\"\n",
        "            f\"Company       : {company_name}\\n\"\n",
        "            f\"Job Type      : {job_type}\\n\"\n",
        "            f\"Job Duration  : {job_duration}\\n\"\n",
        "            f\"Location      : {job_location}\\n\"\n",
        "            f\"Description   : {additional_content}\\n\"\n",
        "            f\"Skills        : {skills}\\n\"\n",
        "            f\"{'-' * 50}\\n\"\n",
        "        )\n",
        "    else:\n",
        "        # Handle multiple roles in the same company\n",
        "        title_container = record.find(\"div\", class_=\"display-flex flex-row justify-space-between\")\n",
        "        company_name_container = title_container.find(\"div\", class_=\"display-flex align-items-center mr1 hoverable-link-text t-bold\") or \\\n",
        "                                 title_container.find(\"div\", class_=\"display-flex align-items-center mr1 t-bold\")\n",
        "        company_name = (\n",
        "            company_name_container.find(\"span\", {\"aria-hidden\": \"true\"}).get_text(strip=True)\n",
        "            if company_name_container else \"N/A\"\n",
        "        )\n",
        "        duration_container = title_container.find(\"span\", class_=\"t-14 t-normal\")\n",
        "        duration = duration_container.find(\"span\", {\"aria-hidden\": \"true\"}).get_text(strip=True)\n",
        "\n",
        "        # Append company details to the output\n",
        "        output += (\n",
        "            f\"Company       : {company_name}\\n\"\n",
        "            f\"Duration      : {duration}\\n\"\n",
        "            f\"{'-' * 50}\\n\"\n",
        "        )\n",
        "\n",
        "        # Process each sub-record (job role)\n",
        "        for sub_record in sub_records:\n",
        "            # Extract job title\n",
        "            job_name_container = sub_record.find(\"div\", class_=\"display-flex align-items-center mr1 hoverable-link-text t-bold\") or \\\n",
        "                                 sub_record.find(\"div\", class_=\"display-flex align-items-center mr1 t-bold\")\n",
        "            job_name = (\n",
        "                job_name_container.find(\"span\", {\"aria-hidden\": \"true\"}).get_text(strip=True)\n",
        "                if job_name_container else \"N/A\"\n",
        "            )\n",
        "\n",
        "            # Extract company name and job type\n",
        "            job_place_and_type_container = sub_record.find(\"span\", class_=\"t-14 t-normal\")\n",
        "            if job_place_and_type_container:\n",
        "                place_and_type = job_place_and_type_container.find(\"span\", {\"aria-hidden\": \"true\"}).get_text(strip=True)\n",
        "                if \"·\" in place_and_type:\n",
        "                    company_name, job_type = [part.strip() for part in place_and_type.split(\"·\", 1)]\n",
        "                else:\n",
        "                    company_name, job_type = \"N/A\", place_and_type\n",
        "            else:\n",
        "                company_name, job_type = \"N/A\", \"N/A\"\n",
        "\n",
        "            # Extract job duration and location\n",
        "            job_info_container = sub_record.find_all(\"span\", class_=\"t-14 t-normal t-black--light\")\n",
        "            info = [span.find(\"span\", {\"aria-hidden\": \"true\"}).get_text(strip=True) for span in job_info_container if span.find(\"span\", {\"aria-hidden\": \"true\"})]\n",
        "            job_duration = info[0] if len(info) > 0 else \"N/A\"\n",
        "            job_location = info[1] if len(info) > 1 else \"N/A\"\n",
        "\n",
        "            # Extract additional content and skills\n",
        "            additional_content_container = sub_record.find_all(\"div\", class_=\"display-flex align-items-center t-14 t-normal t-black\")\n",
        "            if len(additional_content_container) == 1:\n",
        "              if \"Skills:\" in additional_content_container[0].get_text(strip=True):\n",
        "                additional_content_and_skills = [span.find(\"span\", {\"aria-hidden\": \"true\"}).get_text(strip=True) for span in additional_content_container]\n",
        "                skills = additional_content_and_skills[0]\n",
        "                skills = skills.replace(\"Skills:\", \"\").strip()\n",
        "            else:\n",
        "\n",
        "                additional_content_and_skills = [span.find(\"span\", {\"aria-hidden\": \"true\"}).get_text(strip=True) for span in additional_content_container if span.find(\"span\", {\"aria-hidden\": \"true\"})]\n",
        "                additional_content = additional_content_and_skills[0] if len(additional_content_and_skills) > 0 else \"N/A\"\n",
        "\n",
        "                skills = additional_content_and_skills[1] if len(additional_content_and_skills) > 1 else \"N/A\"\n",
        "                skills = skills.replace(\"Skills:\", \"\").strip()\n",
        "\n",
        "            # Append job role details to the output in a clean format\n",
        "            output += (\n",
        "                f\"  Job Title    : {job_name}\\n\"\n",
        "                f\"  Job Type     : {job_type}\\n\"\n",
        "                f\"  Job Duration : {job_duration}\\n\"\n",
        "                f\"  Location     : {job_location}\\n\"\n",
        "                f\"  Description  : {additional_content}\\n\"\n",
        "                f\"  Skills       : {skills}\\n\"\n",
        "                f\"  {'-' * 50}\\n\"\n",
        "            )\n"
      ],
      "metadata": {
        "id": "x81pFF57R_vV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Append the education section header to the output\n",
        "output += \"\\nEducation Section:\\n\\n\"\n",
        "\n",
        "# Locate the education section in the HTML\n",
        "education_section = soup_education.find(\"main\", {\"aria-label\": \"Education\"})\n",
        "\n",
        "# Find all education records within the section\n",
        "education_records = education_section.find_all(\"div\", {\"data-view-name\": \"profile-component-entity\"})\n",
        "\n",
        "# Process each education record\n",
        "for record in education_records:\n",
        "    # Extract institution name\n",
        "    institution_container = record.find(\"div\", class_=\"display-flex align-items-center mr1 hoverable-link-text t-bold\")\n",
        "    institution_name = institution_container.find(\"span\", {\"aria-hidden\": \"true\"}).get_text(strip=True) if institution_container else \"N/A\"\n",
        "    output += f\"Institution     : {institution_name}\\n\"\n",
        "\n",
        "    # Extract date range\n",
        "    date_container = record.find(\"span\", class_=\"pvs-entity__caption-wrapper\")\n",
        "    date_range = date_container.get_text(strip=True) if date_container else \"N/A\"\n",
        "    output += f\"Date Range      : {date_range}\\n\"\n",
        "\n",
        "    # Extract additional description (e.g., degree or field of study)\n",
        "    description_container = record.find(\"span\", class_=\"t-14 t-normal\")\n",
        "    description_text = description_container.find(\"span\", {\"aria-hidden\": \"true\"}).get_text(strip=True) if description_container else \"N/A\"\n",
        "    output += f\"Description     : {description_text}\\n\"\n",
        "\n",
        "    # Extract skills (if available)\n",
        "    skills_container = record.find(\"div\", class_=\"display-flex align-items-center t-14 t-normal t-black\")\n",
        "    skills = []\n",
        "    if skills_container:\n",
        "        skills_span = skills_container.find(\"span\", {\"aria-hidden\": \"true\"})\n",
        "        if skills_span:\n",
        "            skills_text = skills_span.get_text(strip=True)\n",
        "            if \"Skills:\" in skills_text:\n",
        "                skills_text = skills_text.replace(\"Skills:\", \"\").strip()\n",
        "            skills = [skill.strip() for skill in skills_text.split(\"·\")]\n",
        "    output += f\"Skills          : {', '.join(skills) if skills else 'N/A'}\\n\"\n",
        "\n",
        "    # Extract additional text (e.g., detailed explanations)\n",
        "    additional_text_container = record.find(\"div\", class_=\"inline-show-more-text--is-collapsed\")\n",
        "    additional_text = \"\"\n",
        "    if additional_text_container:\n",
        "        additional_span = additional_text_container.find(\"span\", {\"aria-hidden\": \"true\"})\n",
        "        if additional_span:\n",
        "            additional_text = additional_span.get_text(\" \", strip=True)\n",
        "    output += f\"Additional Text : {additional_text if additional_text else 'N/A'}\\n\"\n",
        "\n",
        "    # Append a separator for readability\n",
        "    output += \"-\" * 50 + \"\\n\"\n",
        "\n"
      ],
      "metadata": {
        "id": "zb4BwSHlUmAW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find all <a> tags with data-field=\"skill_page_skill_topic\"\n",
        "skill_links = soup_skill.find_all(\"a\", {\"data-field\": \"skill_page_skill_topic\"})\n",
        "\n",
        "# Extract skills\n",
        "skills = []\n",
        "for link in skill_links:\n",
        "    # Find the <span> with aria-hidden=\"true\" inside the link\n",
        "    skill_span = link.find(\"span\", {\"aria-hidden\": \"true\"})\n",
        "    if skill_span:\n",
        "        skill_text = skill_span.get_text(strip=True)\n",
        "        skills.append(skill_text)\n",
        "\n",
        "unique_skills = sorted(set(skills))\n",
        "\n",
        "output += \"\\nSkills Section:\\n\"\n",
        "output += f\"Unique skills: {', '.join(unique_skills)}\\n\"\n",
        "output += \"-\" * 50 + \"\\n\"\n",
        "\n"
      ],
      "metadata": {
        "id": "KLPVQEfBZ-gW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the output to a text file\n",
        "file_name = \"profile_data.txt\"\n",
        "\n",
        "# Write the output content to the file\n",
        "with open(file_name, \"w\") as file:\n",
        "    file.write(output)\n",
        "\n",
        "# Provide a link to download the file\n",
        "from google.colab import files\n",
        "files.download(file_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "1BjJNv98WTTg",
        "outputId": "4bfa6a97-fef7-4a20-f7b7-3c66fb53f9a9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a89c626f-80cc-446c-98dc-245483bd7cb0\", \"profile_data.txt\", 1132)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from playwright.async_api import async_playwright\n",
        "import asyncio\n",
        "\n",
        "async def scrape_job_posting(job_url):\n",
        "    \"\"\"\n",
        "    Scrape job posting details from a LinkedIn job URL and save to a file.\n",
        "    \"\"\"\n",
        "    async with async_playwright() as p:\n",
        "        # Launch browser in headless mode\n",
        "        browser = await p.chromium.launch(headless=True)\n",
        "        context = await browser.new_context(user_agent=\"Mozilla/5.0\")\n",
        "        page = await context.new_page()\n",
        "\n",
        "        # Navigate to the provided job URL\n",
        "        print(f\"Navigating to {job_url}...\")\n",
        "        await page.goto(job_url)\n",
        "        await asyncio.sleep(5)  # Wait for the page to fully load\n",
        "\n",
        "        # Save the page's HTML content for debugging purposes\n",
        "        # html = await page.content()\n",
        "        # with open(\"page_debug.html\", \"w\", encoding=\"utf-8\") as f:\n",
        "        #     f.write(html)\n",
        "        # print(\"Saved page content to page_debug.html\")\n",
        "\n",
        "        # Take a screenshot of the page for verification\n",
        "        # await page.screenshot(path=\"screenshot.png\", full_page=True)\n",
        "        # print(\"Screenshot saved as screenshot.png\")\n",
        "\n",
        "        # Extract job details using selectors (adjust selectors as needed)\n",
        "        try:\n",
        "            job_title = await page.inner_text('h1.top-card-layout__title')\n",
        "            company = await page.inner_text('a.topcard__org-name-link')\n",
        "            location = await page.inner_text('span.topcard__flavor--bullet')\n",
        "            description = await page.inner_text('div.show-more-less-html__markup')\n",
        "\n",
        "            # Prepare job details for saving\n",
        "            job_details = (\n",
        "                f\"Job Title    : {job_title}\\n\"\n",
        "                f\"Company      : {company}\\n\"\n",
        "                f\"Location     : {location}\\n\"\n",
        "                f\"Description  : {description}\\n\"\n",
        "            )\n",
        "\n",
        "            # Save job details to a text file\n",
        "            output_file = \"job_details.txt\"\n",
        "            with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(job_details)\n",
        "\n",
        "            print(f\"Job details saved to {output_file}\")\n",
        "            return output_file\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred while scraping: {e}\")\n",
        "            return None\n",
        "\n",
        "        finally:\n",
        "            # Close the browser\n",
        "            await browser.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "8QDKUKOnvbv3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the scraping function\n",
        "output_file = await scrape_job_posting(job_url)\n",
        "\n",
        "# Allow user to download the file\n",
        "if output_file:\n",
        "    from google.colab import files\n",
        "    files.download(output_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "e1845xVOvbcJ",
        "outputId": "8f925bf2-0e24-4264-959d-a946502a7a7e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Navigating to https://www.linkedin.com/jobs/view/4114601385/...\n",
            "Job details saved to job_details.txt\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_25a0af6e-ccf1-46cb-b152-08da0c622a5c\", \"job_details.txt\", 1640)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}