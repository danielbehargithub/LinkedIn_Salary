{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielbehargithub/LinkedIn_Salary/blob/main/Scraping/Salary_Scrapping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install playwright\n",
        "!playwright install\n",
        "!pip install anticaptchaofficial\n"
      ],
      "metadata": {
        "id": "WRsd3Vkci2NB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00bbf20a-72b6-43eb-8514-f105a5971242"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: playwright in /usr/local/lib/python3.11/dist-packages (1.49.1)\n",
            "Requirement already satisfied: greenlet==3.1.1 in /usr/local/lib/python3.11/dist-packages (from playwright) (3.1.1)\n",
            "Requirement already satisfied: pyee==12.0.0 in /usr/local/lib/python3.11/dist-packages (from playwright) (12.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from pyee==12.0.0->playwright) (4.12.2)\n",
            "Playwright Host validation warning: \n",
            "╔══════════════════════════════════════════════════════╗\n",
            "║ Host system is missing dependencies to run browsers. ║\n",
            "║ Missing libraries:                                   ║\n",
            "║     libwoff2dec.so.1.0.2                             ║\n",
            "║     libgstgl-1.0.so.0                                ║\n",
            "║     libgstcodecparsers-1.0.so.0                      ║\n",
            "║     libavif.so.13                                    ║\n",
            "║     libharfbuzz-icu.so.0                             ║\n",
            "║     libenchant-2.so.2                                ║\n",
            "║     libsecret-1.so.0                                 ║\n",
            "║     libhyphen.so.0                                   ║\n",
            "║     libmanette-0.2.so.0                              ║\n",
            "╚══════════════════════════════════════════════════════╝\n",
            "    at validateDependenciesLinux (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/dependencies.js:216:9)\n",
            "\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\u001b[39m\n",
            "    at async Registry._validateHostRequirements (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:753:43)\n",
            "    at async Registry._validateHostRequirementsForExecutableIfNeeded (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:851:7)\n",
            "    at async Registry.validateHostRequirementsForExecutablesIfNeeded (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:840:43)\n",
            "    at async t.<anonymous> (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/cli/program.js:137:7)\n",
            "Requirement already satisfied: anticaptchaofficial in /usr/local/lib/python3.11/dist-packages (1.0.62)\n",
            "Requirement already satisfied: py>=1.4.32 in /usr/local/lib/python3.11/dist-packages (from anticaptchaofficial) (1.11.0)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.11/dist-packages (from anticaptchaofficial) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->anticaptchaofficial) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->anticaptchaofficial) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->anticaptchaofficial) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->anticaptchaofficial) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from playwright.async_api import async_playwright\n",
        "import pandas as pd\n",
        "import asyncio\n",
        "\n",
        "async def scrape_payscale():\n",
        "    async with async_playwright() as p:\n",
        "        # Launching a headless Chromium browser\n",
        "        browser = await p.chromium.launch(headless=True)\n",
        "        context = await browser.new_context(\n",
        "            user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "        )\n",
        "        context.set_default_timeout(10000)\n",
        "        page = await context.new_page()\n",
        "\n",
        "        base_url = \"https://www.payscale.com\"\n",
        "        url = f\"{base_url}/research/US/Employer=Intel_Corporation/Salary\"\n",
        "        await page.goto(url)\n",
        "\n",
        "        data = []\n",
        "        page_number = 1\n",
        "\n",
        "        while True:\n",
        "            print(f\"Scraping page {page_number}...\")\n",
        "\n",
        "            # Waiting for the table to load\n",
        "            await asyncio.sleep(10)\n",
        "\n",
        "            # Taking a screenshot of the page for verification\n",
        "            await page.screenshot(path=f\"screenshot_page_{page_number}.png\", full_page=True)\n",
        "            print(f\"Screenshot saved as screenshot_page_{page_number}.png\")\n",
        "\n",
        "            # Extracting table rows\n",
        "            rows = await page.query_selector_all(\"tr.data-table__row\")\n",
        "            print(f\"Found {len(rows)} rows on page {page_number}.\")\n",
        "\n",
        "            for index, row in enumerate(rows):\n",
        "                cols = await row.query_selector_all(\"td\")\n",
        "                if len(cols) >= 3:\n",
        "                    job_title = await cols[0].inner_text()\n",
        "                    salary_range = await cols[1].inner_text()\n",
        "                    salary_average = await cols[2].inner_text()\n",
        "                    data.append((job_title.strip(), salary_range.strip(), salary_average.strip()))\n",
        "                else:\n",
        "                    print(f\"Skipping row {index + 1} on page {page_number} due to insufficient columns.\")\n",
        "\n",
        "            # Check if \"Next\" button exists and is active\n",
        "            next_button = await page.query_selector(\"a.pagination__next-btn:not(.pagination__btn--off)\")\n",
        "            if next_button:\n",
        "                next_page_href = await next_button.get_attribute(\"href\")\n",
        "                next_page_url = f\"{base_url}{next_page_href}\"\n",
        "                await page.goto(next_page_url)\n",
        "                page_number += 1\n",
        "            else:\n",
        "                print(\"No more pages to scrape.\")\n",
        "                break\n",
        "\n",
        "        # Closing the browser\n",
        "        await browser.close()\n",
        "\n",
        "        # Saving the extracted data into a CSV file\n",
        "        if data:\n",
        "            df = pd.DataFrame(data, columns=[\"Job Title\", \"Salary Range\", \"Salary Average\"])\n",
        "            df.to_csv(\"intel_salaries.csv\", index=False)\n",
        "            print(\"Data saved to intel_salaries.csv\")\n",
        "        else:\n",
        "            print(\"No data found to save.\")\n",
        "\n",
        "# Running the async scraping function\n",
        "await scrape_payscale()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGzSvg5OJO4L",
        "outputId": "8e85e991-764c-45d5-d501-754380e2c017"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping page 1...\n",
            "Screenshot saved as screenshot_page_1.png\n",
            "Found 7 rows on page 1.\n",
            "Scraping page 2...\n",
            "Screenshot saved as screenshot_page_2.png\n",
            "Found 7 rows on page 2.\n",
            "Scraping page 3...\n",
            "Screenshot saved as screenshot_page_3.png\n",
            "Found 0 rows on page 3.\n",
            "No more pages to scrape.\n",
            "Data saved to intel_salaries.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TBXu298iLh7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vQjz2jMfLh5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iLUQvKNUL84m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from playwright.async_api import async_playwright\n",
        "import pandas as pd\n",
        "import asyncio\n",
        "import random\n",
        "\n",
        "async def scrape_payscale():\n",
        "    async with async_playwright() as p:\n",
        "        # Launching a headless Chromium browser\n",
        "        browser = await p.chromium.launch(headless=True)\n",
        "        # Creating a browser context with a custom User-Agent\n",
        "        context = await browser.new_context(\n",
        "            user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
        "            viewport={\"width\": random.randint(800, 1920), \"height\": random.randint(600, 1080)},\n",
        "            locale=\"en-US\",\n",
        "            timezone_id=\"America/New_York\"\n",
        "        )\n",
        "        # Setting a default timeout for all operations\n",
        "        context.set_default_timeout(10000)\n",
        "        page = await context.new_page()\n",
        "\n",
        "        base_url = \"https://www.payscale.com\"\n",
        "        url = f\"{base_url}/research/US/Employer=Intel_Corporation/Salary\"\n",
        "        await page.goto(url)\n",
        "\n",
        "        data = []\n",
        "        page_number = 1\n",
        "\n",
        "        while True:\n",
        "            print(f\"Scraping page {page_number}...\")\n",
        "\n",
        "            # Adding random delay to mimic human behavior\n",
        "            delay = random.uniform(3, 7)\n",
        "            print(f\"Adding random delay of {delay:.2f} seconds...\")\n",
        "            await asyncio.sleep(delay)\n",
        "\n",
        "            # Scrolling randomly down the page to simulate human interaction\n",
        "            scroll_distance = random.randint(500, 1500)\n",
        "            print(f\"Scrolling down {scroll_distance} pixels...\")\n",
        "            await page.evaluate(f\"window.scrollBy(0, {scroll_distance});\")\n",
        "            await asyncio.sleep(random.uniform(1, 3))\n",
        "\n",
        "            # Clicking random elements (if present)\n",
        "            random_clickable = await page.query_selector_all(\"a, button\")\n",
        "            if random_clickable:\n",
        "                element_to_click = random.choice(random_clickable)\n",
        "                # Check if the element is visible and enabled before clicking\n",
        "                is_visible = await element_to_click.is_visible()\n",
        "                is_enabled = await element_to_click.is_enabled()\n",
        "                if is_visible and is_enabled:\n",
        "                    print(\"Clicking a random element to simulate interaction...\")\n",
        "                    await element_to_click.click()\n",
        "                    await asyncio.sleep(random.uniform(2, 5))\n",
        "                else:\n",
        "                    print(\"Skipped clicking as the element is not visible or enabled.\")\n",
        "\n",
        "            # Typing random text into search fields (if present)\n",
        "            search_field = await page.query_selector(\"input[type='text']\")\n",
        "            if search_field:\n",
        "                random_text = ''.join(random.choices(\"abcdefghijklmnopqrstuvwxyz\", k=random.randint(5, 10)))\n",
        "                print(f\"Typing random text '{random_text}' into search field...\")\n",
        "                await search_field.type(random_text, delay=random.uniform(50, 150))  # Simulate typing speed\n",
        "                await asyncio.sleep(random.uniform(2, 4))\n",
        "                print(\"Clearing search field...\")\n",
        "                await search_field.fill(\"\")  # Clear the field\n",
        "\n",
        "            # Taking a screenshot of the page for verification\n",
        "            await page.screenshot(path=f\"screenshot_page_{page_number}.png\", full_page=True)\n",
        "            print(f\"Screenshot saved as screenshot_page_{page_number}.png\")\n",
        "\n",
        "            # Extracting table rows\n",
        "            rows = await page.query_selector_all(\"tr.data-table__row\")\n",
        "            print(f\"Found {len(rows)} rows on page {page_number}.\")\n",
        "\n",
        "            for index, row in enumerate(rows):\n",
        "                cols = await row.query_selector_all(\"td\")\n",
        "                if len(cols) >= 3:\n",
        "                    job_title = await cols[0].inner_text()\n",
        "                    salary_range = await cols[1].inner_text()\n",
        "                    salary_average = await cols[2].inner_text()\n",
        "                    data.append((job_title.strip(), salary_range.strip(), salary_average.strip()))\n",
        "                else:\n",
        "                    print(f\"Skipping row {index + 1} on page {page_number} due to insufficient columns.\")\n",
        "\n",
        "            # Check if \"Next\" button exists and is active\n",
        "            next_button = await page.query_selector(\"a.pagination__next-btn:not(.pagination__btn--off)\")\n",
        "            if next_button:\n",
        "                next_page_href = await next_button.get_attribute(\"href\")\n",
        "                next_page_url = f\"{base_url}{next_page_href}\"\n",
        "                await page.goto(next_page_url)\n",
        "                page_number += 1\n",
        "            else:\n",
        "                print(\"No more pages to scrape.\")\n",
        "                break\n",
        "\n",
        "        # Closing the browser\n",
        "        await browser.close()\n",
        "\n",
        "        # Saving the extracted data into a CSV file\n",
        "        if data:\n",
        "            df = pd.DataFrame(data, columns=[\"Job Title\", \"Salary Range\", \"Salary Average\"])\n",
        "            df.to_csv(\"intel_salaries.csv\", index=False)\n",
        "            print(\"Data saved to intel_salaries.csv\")\n",
        "        else:\n",
        "            print(\"No data found to save.\")\n",
        "\n",
        "# Running the async scraping function\n",
        "await scrape_payscale()\n"
      ],
      "metadata": {
        "id": "ISpbxX6nNLco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dLIUvG9XWin8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from playwright.async_api import async_playwright\n",
        "\n",
        "async def scrape_with_bright_data():\n",
        "    async with async_playwright() as p:\n",
        "        # Connect to the remote browser via WebSocket\n",
        "        browser = await p.chromium.connect_over_cdp(\"wss://brd-customer-hl_80709a30-zone-aviata_scraping:4k44h8n2qgoc@brd.superproxy.io:9222\")\n",
        "\n",
        "        # Open a new page\n",
        "        context = await browser.new_context()\n",
        "        page = await context.new_page()\n",
        "\n",
        "        # Navigate to the target URL\n",
        "        await page.goto(\"https://www.payscale.com/research/US/Employer=Intel_Corporation/Salary\")\n",
        "\n",
        "        # Scrape data or interact with the page\n",
        "        print(\"Page title:\", await page.title())\n",
        "\n",
        "        # Close browser\n",
        "        await browser.close()\n",
        "\n",
        "# Run the scraping function directly with await\n",
        "await scrape_with_bright_data()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        },
        "id": "HlxQvhDpZkre",
        "outputId": "69809ad3-8067-407f-b1b1-b61e9c5f0bdb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Error",
          "evalue": "BrowserType.connect_over_cdp: WebSocket error: wss://brd-customer-hl_80709a30-zone-aviata_scraping:4k44h8n2qgoc@brd.superproxy.io:9222/ 403 Auth Failed (customer_suspended)\nAccount is suspended\nCall log:\n  - <ws connecting> wss://brd-customer-hl_80709a30-zone-aviata_scraping:4k44h8n2qgoc@brd.superproxy.io:9222/\n  -   - <ws unexpected response> wss://brd-customer-hl_80709a30-zone-aviata_scraping:4k44h8n2qgoc@brd.superproxy.io:9222/ 403 Auth Failed (customer_suspended)\nAccount is suspended\n  -   - <ws error> wss://brd-customer-hl_80709a30-zone-aviata_scraping:4k44h8n2qgoc@brd.superproxy.io:9222/ error WebSocket was closed before the connection was established\n  -   - <ws connect error> wss://brd-customer-hl_80709a30-zone-aviata_scraping:4k44h8n2qgoc@brd.superproxy.io:9222/ WebSocket was closed before the connection was established\n  -   - <ws disconnected> wss://brd-customer-hl_80709a30-zone-aviata_scraping:4k44h8n2qgoc@brd.superproxy.io:9222/ code=1006 reason=\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-64231cb6843f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Run the scraping function directly with await\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mawait\u001b[0m \u001b[0mscrape_with_bright_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-64231cb6843f>\u001b[0m in \u001b[0;36mscrape_with_bright_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0masync_playwright\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# Connect to the remote browser via WebSocket\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mbrowser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchromium\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect_over_cdp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"wss://brd-customer-hl_80709a30-zone-aviata_scraping:4k44h8n2qgoc@brd.superproxy.io:9222\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# Open a new page\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/playwright/async_api/_generated.py\u001b[0m in \u001b[0;36mconnect_over_cdp\u001b[0;34m(self, endpoint_url, timeout, slow_mo, headers)\u001b[0m\n\u001b[1;32m  14777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  14778\u001b[0m         return mapping.from_impl(\n\u001b[0;32m> 14779\u001b[0;31m             await self._impl_obj.connect_over_cdp(\n\u001b[0m\u001b[1;32m  14780\u001b[0m                 \u001b[0mendpointURL\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  14781\u001b[0m                 \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/playwright/_impl/_browser_type.py\u001b[0m in \u001b[0;36mconnect_over_cdp\u001b[0;34m(self, endpointURL, timeout, slowMo, headers)\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"headers\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"headers\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mserialize_headers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"headers\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_channel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_return_as_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"connectOverCDP\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0mbrowser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBrowser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_channel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"browser\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_did_launch_browser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbrowser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/playwright/_impl/_connection.py\u001b[0m in \u001b[0;36msend_return_as_dict\u001b[0;34m(self, method, params)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msend_return_as_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         return await self._connection.wrap_api_call(\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_send\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_internal_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/playwright/_impl/_connection.py\u001b[0m in \u001b[0;36mwrap_api_call\u001b[0;34m(self, cb, is_internal)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mrewrite_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{parsed_st['apiName']}: {error}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api_zone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mError\u001b[0m: BrowserType.connect_over_cdp: WebSocket error: wss://brd-customer-hl_80709a30-zone-aviata_scraping:4k44h8n2qgoc@brd.superproxy.io:9222/ 403 Auth Failed (customer_suspended)\nAccount is suspended\nCall log:\n  - <ws connecting> wss://brd-customer-hl_80709a30-zone-aviata_scraping:4k44h8n2qgoc@brd.superproxy.io:9222/\n  -   - <ws unexpected response> wss://brd-customer-hl_80709a30-zone-aviata_scraping:4k44h8n2qgoc@brd.superproxy.io:9222/ 403 Auth Failed (customer_suspended)\nAccount is suspended\n  -   - <ws error> wss://brd-customer-hl_80709a30-zone-aviata_scraping:4k44h8n2qgoc@brd.superproxy.io:9222/ error WebSocket was closed before the connection was established\n  -   - <ws connect error> wss://brd-customer-hl_80709a30-zone-aviata_scraping:4k44h8n2qgoc@brd.superproxy.io:9222/ WebSocket was closed before the connection was established\n  -   - <ws disconnected> wss://brd-customer-hl_80709a30-zone-aviata_scraping:4k44h8n2qgoc@brd.superproxy.io:9222/ code=1006 reason=\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "afWE5x87fp_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pIAJelAkfp9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = 'https://www.glassdoor.com/Salaries/data-scientist-salary-SRCH_KO0,16.htm'\n",
        "\n",
        "response = requests.get(url)\n",
        "\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "salaries = soup.find_all('div', class_='salarySnippet')\n",
        "\n",
        "for salary in salaries:\n",
        "\n",
        "  name = salary.find('h3').text\n",
        "\n",
        "  location = salary.find('span', class_='location').text\n",
        "\n",
        "  salary_range = salary.find('span', class_='value').text\n",
        "\n",
        "  print(f'Job Title: {name}')\n",
        "\n",
        "  print(f'Location: {location}')\n",
        "\n",
        "  print(f'Salary Range: {salary_range}')\n",
        "\n",
        "  print(f'Scraped {len(salaries)} salaries from Glassdoor')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixhMJQiBfp6B",
        "outputId": "a5af4ad6-05e6-40c9-ef0a-f101c0e6bc27"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.11/collections/__init__.py:474: RuntimeWarning: coroutine 'scrape_with_bright_data' was never awaited\n",
            "  for method in (\n",
            "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
          ]
        }
      ]
    }
  ]
}
