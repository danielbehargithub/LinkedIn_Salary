{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyMWKwOp9jqfHmBH+89Vf/1V",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/danielbehargithub/LinkedIn_Salary/blob/main/Salary_Scrapping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install playwright\n",
    "!playwright install\n",
    "!pip install anticaptchaofficial\n"
   ],
   "metadata": {
    "id": "WRsd3Vkci2NB"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from playwright.async_api import async_playwright\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "\n",
    "async def scrape_payscale():\n",
    "    async with async_playwright() as p:\n",
    "        # Launching a headless Chromium browser\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        # Creating a browser context with a custom User-Agent\n",
    "        context = await browser.new_context(\n",
    "            user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "        )\n",
    "        # Setting a default timeout for all operations\n",
    "        context.set_default_timeout(10000)\n",
    "        page = await context.new_page()\n",
    "\n",
    "        # Navigating to the target URL\n",
    "        url = \"https://www.payscale.com/research/US/Employer=Intel_Corporation/Salary\"\n",
    "        print(\"Navigating to URL...\")\n",
    "        response = await page.goto(url)\n",
    "        # Check if the page was loaded successfully\n",
    "        if response.status != 200:\n",
    "            print(f\"Failed to load page. Status code: {response.status}\")\n",
    "            return\n",
    "\n",
    "        # Adding a sleep to ensure the page is fully loaded\n",
    "        print(\"Waiting for page to load...\")\n",
    "        await asyncio.sleep(5)\n",
    "\n",
    "        # Saving the page's HTML content for debugging purposes\n",
    "        html = await page.content()\n",
    "        with open(\"page_debug.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(html)\n",
    "        print(\"Saved page content to page_debug.html\")\n",
    "\n",
    "        # Taking a screenshot of the page for verification\n",
    "        await page.screenshot(path=\"screenshot.png\", full_page=True)\n",
    "        print(\"Screenshot saved as screenshot.png\")\n",
    "\n",
    "        # Extracting table rows using a specific selector\n",
    "        print(\"Extracting table rows...\")\n",
    "        rows = await page.query_selector_all(\"tr.data-table__row\")\n",
    "        print(f\"Found {len(rows)} rows in the table.\")\n",
    "\n",
    "        data = []\n",
    "        # Iterating through each table row to extract job and salary information\n",
    "        for index, row in enumerate(rows):\n",
    "            print(f\"Processing row {index + 1}...\")\n",
    "            # Extracting columns (td elements) within the row\n",
    "            cols = await row.query_selector_all(\"td\")\n",
    "            if len(cols) >= 3:  # Ensure the row has at least 3 columns\n",
    "                job_title = await cols[0].inner_text()\n",
    "                salary_range = await cols[1].inner_text()\n",
    "                salary_average = await cols[2].inner_text()\n",
    "                data.append((job_title.strip(), salary_range.strip(), salary_average.strip()))\n",
    "            else:\n",
    "                print(f\"Skipping row {index + 1} due to insufficient columns.\")\n",
    "\n",
    "        # Closing the browser\n",
    "        await browser.close()\n",
    "\n",
    "        # Saving the extracted data into a CSV file\n",
    "        if data:\n",
    "            df = pd.DataFrame(data, columns=[\"Job Title\", \"Salary Range\", \"Salary Average\"])\n",
    "            df.to_csv(\"intel_salaries.csv\", index=False)\n",
    "            print(\"Data saved to intel_salaries.csv\")\n",
    "        else:\n",
    "            print(\"No data found to save.\")\n",
    "\n",
    "# Running the async scraping function\n",
    "await scrape_payscale()\n",
    ""
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PdvthsKIHX0q",
    "outputId": "eed0bc86-cfcf-4227-be7c-1fe5994591af"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Navigating to URL...\n",
      "Waiting for page to load...\n",
      "Saved page content to page_debug.html\n",
      "Screenshot saved as screenshot.png\n",
      "Extracting table rows...\n",
      "Found 7 rows in the table.\n",
      "Processing row 1...\n",
      "Processing row 2...\n",
      "Processing row 3...\n",
      "Processing row 4...\n",
      "Processing row 5...\n",
      "Processing row 6...\n",
      "Processing row 7...\n",
      "Data saved to intel_salaries.csv\n"
     ]
    }
   ]
  }
 ]
}
