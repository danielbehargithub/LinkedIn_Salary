{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMWKwOp9jqfHmBH+89Vf/1V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielbehargithub/LinkedIn_Salary/blob/main/Salary_Scrapping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_q9LBoR1ThP",
        "outputId": "e4d16924-9809-4ca7-9499-81d80f1e64ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No table found on the page.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "def check_robots_txt(url):\n",
        "    \"\"\"בודק אם מותר לגשת לפי robots.txt\"\"\"\n",
        "    robots_url = \"/\".join(url.split(\"/\")[:3]) + \"/robots.txt\"\n",
        "    response = requests.get(robots_url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
        "    if response.status_code == 200:\n",
        "        disallowed_paths = response.text.splitlines()\n",
        "        for line in disallowed_paths:\n",
        "            if line.startswith(\"Disallow\"):\n",
        "                disallowed_path = line.split(\": \")[1].strip()\n",
        "                if disallowed_path in url:\n",
        "                    return False\n",
        "    return True\n",
        "\n",
        "def scrape_intel_jobs(url):\n",
        "    \"\"\"מבצע סקרייפינג לטבלת השכר של אינטל\"\"\"\n",
        "    # בדיקת robots.txt\n",
        "    if not check_robots_txt(url):\n",
        "        return \"Access denied by robots.txt\"\n",
        "\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
        "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
        "        \"Connection\": \"keep-alive\",\n",
        "    }\n",
        "\n",
        "    response = requests.get(url, headers=headers)\n",
        "\n",
        "    if response.status_code == 403:\n",
        "        return \"Failed to access page. Status code: 403 - Access forbidden\"\n",
        "    elif response.status_code != 200:\n",
        "        return f\"Failed to access page. Status code: {response.status_code}\"\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # מציאת הטבלה שמכילה את המידע\n",
        "    table = soup.find('table', {'class': 'data-table'})  # זיהוי לפי מחלקה\n",
        "    if not table:\n",
        "        return \"No table found on the page.\"\n",
        "\n",
        "    # חילוץ נתונים מהטבלה\n",
        "    rows = table.find_all('tr')\n",
        "    data = []\n",
        "    for row in rows[1:]:  # דילוג על כותרות\n",
        "        cols = row.find_all('td')\n",
        "        if len(cols) >= 3:  # אם יש לפחות 3 עמודות (תפקיד, מינימום, מקסימום)\n",
        "            job_title = cols[0].text.strip()\n",
        "            salary_min = cols[1].text.strip()\n",
        "            salary_max = cols[2].text.strip()\n",
        "            data.append((job_title, salary_min, salary_max))\n",
        "\n",
        "    return data\n",
        "\n",
        "# כתובת URL של הדף\n",
        "url = \"https://www.payscale.com/research/US/Employer=Intel_Corporation/Salary\"\n",
        "\n",
        "# קריאה לפונקציה\n",
        "data = scrape_intel_jobs(url)\n",
        "if isinstance(data, str):\n",
        "    print(data)  # הודעת שגיאה או גישה חסומה\n",
        "else:\n",
        "    # הדפסה של התוצאות או שמירה בקובץ CSV\n",
        "    df = pd.DataFrame(data, columns=[\"Job Title\", \"Salary Min\", \"Salary Max\"])\n",
        "    df.to_csv(\"intel_salaries.csv\", index=False)\n",
        "    print(\"Data saved to intel_salaries.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install playwright\n",
        "!playwright install\n",
        "!pip install anticaptchaofficial\n"
      ],
      "metadata": {
        "id": "WRsd3Vkci2NB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from playwright.async_api import async_playwright\n",
        "import pandas as pd\n",
        "import asyncio\n",
        "\n",
        "async def scrape_payscale():\n",
        "    async with async_playwright() as p:\n",
        "        # Launching a headless Chromium browser\n",
        "        browser = await p.chromium.launch(headless=True)\n",
        "        # Creating a browser context with a custom User-Agent\n",
        "        context = await browser.new_context(\n",
        "            user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "        )\n",
        "        # Setting a default timeout for all operations\n",
        "        context.set_default_timeout(10000)\n",
        "        page = await context.new_page()\n",
        "\n",
        "        # Navigating to the target URL\n",
        "        url = \"https://www.payscale.com/research/US/Employer=Intel_Corporation/Salary\"\n",
        "        print(\"Navigating to URL...\")\n",
        "        response = await page.goto(url)\n",
        "        # Check if the page was loaded successfully\n",
        "        if response.status != 200:\n",
        "            print(f\"Failed to load page. Status code: {response.status}\")\n",
        "            return\n",
        "\n",
        "        # Adding a sleep to ensure the page is fully loaded\n",
        "        print(\"Waiting for page to load...\")\n",
        "        await asyncio.sleep(5)\n",
        "\n",
        "        # Saving the page's HTML content for debugging purposes\n",
        "        html = await page.content()\n",
        "        with open(\"page_debug.html\", \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(html)\n",
        "        print(\"Saved page content to page_debug.html\")\n",
        "\n",
        "        # Taking a screenshot of the page for verification\n",
        "        await page.screenshot(path=\"screenshot.png\", full_page=True)\n",
        "        print(\"Screenshot saved as screenshot.png\")\n",
        "\n",
        "        # Extracting table rows using a specific selector\n",
        "        print(\"Extracting table rows...\")\n",
        "        rows = await page.query_selector_all(\"tr.data-table__row\")\n",
        "        print(f\"Found {len(rows)} rows in the table.\")\n",
        "\n",
        "        data = []\n",
        "        # Iterating through each table row to extract job and salary information\n",
        "        for index, row in enumerate(rows):\n",
        "            print(f\"Processing row {index + 1}...\")\n",
        "            # Extracting columns (td elements) within the row\n",
        "            cols = await row.query_selector_all(\"td\")\n",
        "            if len(cols) >= 3:  # Ensure the row has at least 3 columns\n",
        "                job_title = await cols[0].inner_text()\n",
        "                salary_range = await cols[1].inner_text()\n",
        "                salary_average = await cols[2].inner_text()\n",
        "                data.append((job_title.strip(), salary_range.strip(), salary_average.strip()))\n",
        "            else:\n",
        "                print(f\"Skipping row {index + 1} due to insufficient columns.\")\n",
        "\n",
        "        # Closing the browser\n",
        "        await browser.close()\n",
        "\n",
        "        # Saving the extracted data into a CSV file\n",
        "        if data:\n",
        "            df = pd.DataFrame(data, columns=[\"Job Title\", \"Salary Range\", \"Salary Average\"])\n",
        "            df.to_csv(\"intel_salaries.csv\", index=False)\n",
        "            print(\"Data saved to intel_salaries.csv\")\n",
        "        else:\n",
        "            print(\"No data found to save.\")\n",
        "\n",
        "# Running the async scraping function\n",
        "await scrape_payscale()\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdvthsKIHX0q",
        "outputId": "eed0bc86-cfcf-4227-be7c-1fe5994591af"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Navigating to URL...\n",
            "Waiting for page to load...\n",
            "Saved page content to page_debug.html\n",
            "Screenshot saved as screenshot.png\n",
            "Extracting table rows...\n",
            "Found 7 rows in the table.\n",
            "Processing row 1...\n",
            "Processing row 2...\n",
            "Processing row 3...\n",
            "Processing row 4...\n",
            "Processing row 5...\n",
            "Processing row 6...\n",
            "Processing row 7...\n",
            "Data saved to intel_salaries.csv\n"
          ]
        }
      ]
    }
  ]
}